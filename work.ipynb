{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3936afa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 13:26:00,327 - INFO - Navigating to LinkedIn login page\n",
      "2025-06-20 13:26:15,049 - INFO - Successfully logged in\n",
      "2025-06-20 13:26:20,265 - INFO - Navigating to activity page: https://www.linkedin.com/in/simonsinek/recent-activity/all/\n",
      "2025-06-20 13:26:28,852 - INFO - Found posts with selector: .occludable-update\n",
      "2025-06-20 13:26:30,864 - INFO - Scrolled to top of page\n",
      "2025-06-20 13:26:31,087 - INFO - Scroll 1/15\n",
      "2025-06-20 13:26:31,106 - INFO - Found 0 potential 'see more' buttons with selector: .inline-show-more-text__button\n",
      "2025-06-20 13:26:31,116 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-inline-show-more-text__see-more\n",
      "2025-06-20 13:26:31,125 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-text-view__see-more\n",
      "2025-06-20 13:26:31,134 - INFO - Found 4 potential 'see more' buttons with selector: .see-more\n",
      "2025-06-20 13:26:32,289 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:26:34,342 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:26:36,396 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:26:38,447 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:26:39,475 - INFO - Found 0 potential 'see more' buttons with selector: span.lt-line-clamp__more\n",
      "2025-06-20 13:26:39,508 - INFO - Expanded 31 'see more' buttons with JavaScript\n",
      "2025-06-20 13:26:42,555 - INFO - Found 5 new posts. Total accumulated: 5\n",
      "2025-06-20 13:26:45,812 - INFO - Scroll 2/15\n",
      "2025-06-20 13:26:45,837 - INFO - Found 0 potential 'see more' buttons with selector: .inline-show-more-text__button\n",
      "2025-06-20 13:26:45,852 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-inline-show-more-text__see-more\n",
      "2025-06-20 13:26:45,862 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-text-view__see-more\n",
      "2025-06-20 13:26:45,874 - INFO - Found 1 potential 'see more' buttons with selector: .see-more\n",
      "2025-06-20 13:26:46,931 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:26:47,956 - INFO - Found 0 potential 'see more' buttons with selector: span.lt-line-clamp__more\n",
      "2025-06-20 13:26:49,448 - INFO - Found 2 new posts. Total accumulated: 7\n",
      "2025-06-20 13:26:52,471 - INFO - Scroll 3/15\n",
      "2025-06-20 13:26:52,509 - INFO - Found 0 potential 'see more' buttons with selector: .inline-show-more-text__button\n",
      "2025-06-20 13:26:52,519 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-inline-show-more-text__see-more\n",
      "2025-06-20 13:26:52,529 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-text-view__see-more\n",
      "2025-06-20 13:26:52,540 - INFO - Found 2 potential 'see more' buttons with selector: .see-more\n",
      "2025-06-20 13:26:53,590 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:26:55,671 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:26:56,695 - INFO - Found 0 potential 'see more' buttons with selector: span.lt-line-clamp__more\n",
      "2025-06-20 13:26:56,748 - INFO - Expanded 31 'see more' buttons with JavaScript\n",
      "2025-06-20 13:27:00,718 - INFO - Found 3 new posts. Total accumulated: 10\n",
      "2025-06-20 13:27:03,740 - INFO - Scroll 4/15\n",
      "2025-06-20 13:27:03,762 - INFO - Found 0 potential 'see more' buttons with selector: .inline-show-more-text__button\n",
      "2025-06-20 13:27:03,777 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-inline-show-more-text__see-more\n",
      "2025-06-20 13:27:03,794 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-text-view__see-more\n",
      "2025-06-20 13:27:03,806 - INFO - Found 1 potential 'see more' buttons with selector: .see-more\n",
      "2025-06-20 13:27:04,842 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:27:05,865 - INFO - Found 0 potential 'see more' buttons with selector: span.lt-line-clamp__more\n",
      "2025-06-20 13:27:05,940 - INFO - Expanded 31 'see more' buttons with JavaScript\n",
      "2025-06-20 13:27:10,873 - INFO - Found 2 new posts. Total accumulated: 12\n",
      "2025-06-20 13:27:14,135 - INFO - Scroll 5/15\n",
      "2025-06-20 13:27:14,151 - INFO - Found 0 potential 'see more' buttons with selector: .inline-show-more-text__button\n",
      "2025-06-20 13:27:14,160 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-inline-show-more-text__see-more\n",
      "2025-06-20 13:27:14,176 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-text-view__see-more\n",
      "2025-06-20 13:27:14,190 - INFO - Found 0 potential 'see more' buttons with selector: .see-more\n",
      "2025-06-20 13:27:14,202 - INFO - Found 0 potential 'see more' buttons with selector: span.lt-line-clamp__more\n",
      "2025-06-20 13:27:16,564 - INFO - Found 0 new posts. Total accumulated: 12\n",
      "2025-06-20 13:27:19,586 - INFO - Scroll 6/15\n",
      "2025-06-20 13:27:19,620 - INFO - Found 0 potential 'see more' buttons with selector: .inline-show-more-text__button\n",
      "2025-06-20 13:27:19,639 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-inline-show-more-text__see-more\n",
      "2025-06-20 13:27:19,651 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-text-view__see-more\n",
      "2025-06-20 13:27:19,660 - INFO - Found 3 potential 'see more' buttons with selector: .see-more\n",
      "2025-06-20 13:27:20,720 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:27:22,803 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:27:24,870 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:27:25,910 - INFO - Found 0 potential 'see more' buttons with selector: span.lt-line-clamp__more\n",
      "2025-06-20 13:27:29,242 - INFO - Found 3 new posts. Total accumulated: 15\n",
      "2025-06-20 13:27:32,279 - INFO - Scroll 7/15\n",
      "2025-06-20 13:27:32,308 - INFO - Found 0 potential 'see more' buttons with selector: .inline-show-more-text__button\n",
      "2025-06-20 13:27:32,338 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-inline-show-more-text__see-more\n",
      "2025-06-20 13:27:32,365 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-text-view__see-more\n",
      "2025-06-20 13:27:32,375 - INFO - Found 0 potential 'see more' buttons with selector: .see-more\n",
      "2025-06-20 13:27:32,384 - INFO - Found 0 potential 'see more' buttons with selector: span.lt-line-clamp__more\n",
      "2025-06-20 13:27:36,104 - INFO - Found 0 new posts. Total accumulated: 15\n",
      "2025-06-20 13:27:39,313 - INFO - Scroll 8/15\n",
      "2025-06-20 13:27:39,325 - INFO - Found 0 potential 'see more' buttons with selector: .inline-show-more-text__button\n",
      "2025-06-20 13:27:39,334 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-inline-show-more-text__see-more\n",
      "2025-06-20 13:27:39,341 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-text-view__see-more\n",
      "2025-06-20 13:27:39,349 - INFO - Found 5 potential 'see more' buttons with selector: .see-more\n",
      "2025-06-20 13:27:40,419 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:27:42,502 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:27:44,593 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:27:46,678 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:27:48,760 - INFO - Expanded post with JS click\n",
      "2025-06-20 13:27:49,795 - INFO - Found 0 potential 'see more' buttons with selector: span.lt-line-clamp__more\n",
      "2025-06-20 13:27:49,889 - INFO - Expanded 31 'see more' buttons with JavaScript\n",
      "2025-06-20 13:27:56,931 - INFO - Found 7 new posts. Total accumulated: 22\n",
      "2025-06-20 13:27:56,932 - INFO - Reached target of 20 posts\n",
      "2025-06-20 13:27:56,940 - INFO - Found 0 potential 'see more' buttons with selector: .inline-show-more-text__button\n",
      "2025-06-20 13:27:56,952 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-inline-show-more-text__see-more\n",
      "2025-06-20 13:27:56,968 - INFO - Found 0 potential 'see more' buttons with selector: .feed-shared-text-view__see-more\n",
      "2025-06-20 13:27:56,978 - INFO - Found 0 potential 'see more' buttons with selector: .see-more\n",
      "2025-06-20 13:27:56,985 - INFO - Found 0 potential 'see more' buttons with selector: span.lt-line-clamp__more\n",
      "2025-06-20 13:28:01,863 - INFO - Successfully completed scraping. Total posts: 22\n",
      "2025-06-20 13:28:01,868 - INFO - Saved 22 posts to data/linkedin_posts_Leadership_20250620_132801.csv\n",
      "2025-06-20 13:28:01,870 - INFO - Backup saved to data/linkedin_posts_Leadership_7678bb24.csv\n",
      "2025-06-20 13:28:01,871 - INFO - Successfully saved 22 posts from profile\n",
      "2025-06-20 13:28:01,873 - INFO - Saved 22 posts to scraped_posts.csv\n",
      "2025-06-20 13:28:04,147 - INFO - Browser closed successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from dotenv import load_dotenv\n",
    "from uuid import uuid4\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LinkedInScraper:\n",
    "    def __init__(self, headless=False, debug=True, max_posts=50):\n",
    "        \"\"\"Initialize the LinkedIn scraper with login credentials.\"\"\"\n",
    "        self.email = os.getenv(\"LINKEDIN_EMAIL\")\n",
    "        self.password = os.getenv(\"LINKEDIN_PASSWORD\")\n",
    "        self.debug = debug\n",
    "        self.max_posts = max_posts\n",
    "        self.session_id = str(uuid4())[:8]  # Generate a unique session ID\n",
    "        \n",
    "        # Create debug directory\n",
    "        if self.debug:\n",
    "            os.makedirs('debug', exist_ok=True)\n",
    "        \n",
    "        # Create data directory\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        \n",
    "        # Setup Selenium options\n",
    "        options = webdriver.ChromeOptions()\n",
    "        if headless:\n",
    "            options.add_argument('--headless=new')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-notifications')\n",
    "        options.add_argument('--window-size=1920,1080')\n",
    "        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "        \n",
    "        # Initialize the driver\n",
    "        self.driver = webdriver.Chrome(options=options)\n",
    "        self.wait = WebDriverWait(self.driver, 15)\n",
    "        self.logged_in = False\n",
    "        \n",
    "        # Add a variable to track accumulated posts during scraping\n",
    "        self.accumulated_posts = []\n",
    "    \n",
    "    def login(self):\n",
    "        \"\"\"Log in to LinkedIn.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Navigating to LinkedIn login page\")\n",
    "            self.driver.get('https://www.linkedin.com/login')\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Take screenshot of login page\n",
    "            if self.debug:\n",
    "                self.driver.save_screenshot(f'debug/{self.session_id}_login_page.png')\n",
    "            \n",
    "            # Wait for login page to load\n",
    "            self.wait.until(EC.presence_of_element_located((By.ID, 'username')))\n",
    "            \n",
    "            # Enter email\n",
    "            username_field = self.driver.find_element(By.ID, 'username')\n",
    "            username_field.clear()\n",
    "            username_field.send_keys(self.email)\n",
    "            \n",
    "            # Enter password\n",
    "            password_field = self.driver.find_element(By.ID, 'password')\n",
    "            password_field.clear()\n",
    "            password_field.send_keys(self.password)\n",
    "            \n",
    "            # Click the login button\n",
    "            self.driver.find_element(By.CSS_SELECTOR, 'button[type=\"submit\"]').click()\n",
    "            \n",
    "            # Wait for the homepage to load\n",
    "            try:\n",
    "                self.wait.until(EC.presence_of_element_located((By.ID, 'global-nav')))\n",
    "                logger.info(\"Successfully logged in\")\n",
    "                self.logged_in = True\n",
    "                \n",
    "                # Take screenshot after login\n",
    "                if self.debug:\n",
    "                    self.driver.save_screenshot(f'debug/{self.session_id}_after_login.png')\n",
    "                \n",
    "                # Wait a bit after login\n",
    "                time.sleep(5)\n",
    "                \n",
    "            except TimeoutException:\n",
    "                # Check if we got a security verification page\n",
    "                if \"security verification\" in self.driver.page_source.lower() or \"challenge\" in self.driver.page_source.lower():\n",
    "                    logger.warning(\"Security verification detected. Please complete it manually.\")\n",
    "                    if self.debug:\n",
    "                        self.driver.save_screenshot(f'debug/{self.session_id}_security_verification.png')\n",
    "                    input(\"Complete the security verification and press Enter to continue...\")\n",
    "                    self.logged_in = True\n",
    "                else:\n",
    "                    logger.error(\"Login failed - couldn't detect navigation bar\")\n",
    "                    if self.debug:\n",
    "                        self.driver.save_screenshot(f'debug/{self.session_id}_login_failure.png')\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to login: {str(e)}\")\n",
    "            if self.debug:\n",
    "                self.driver.save_screenshot(f'debug/{self.session_id}_login_error.png')\n",
    "            raise e\n",
    "    \n",
    "    def navigate_to_profile(self, profile_url):\n",
    "        \"\"\"Navigate to a LinkedIn profile and ensure it's loaded.\"\"\"\n",
    "        if not self.logged_in:\n",
    "            self.login()\n",
    "        \n",
    "        try:\n",
    "            # Ensure URL is the recent-activity/all page\n",
    "            if \"recent-activity/all\" not in profile_url:\n",
    "                if not profile_url.endswith('/'):\n",
    "                    profile_url = profile_url + '/'\n",
    "                profile_url = profile_url + \"recent-activity/all/\"\n",
    "            \n",
    "            logger.info(f\"Navigating to activity page: {profile_url}\")\n",
    "            self.driver.get(profile_url)\n",
    "            \n",
    "            # Wait for page to load\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Take screenshot of profile page\n",
    "            if self.debug:\n",
    "                self.driver.save_screenshot(f'debug/{self.session_id}_profile_page.png')\n",
    "            \n",
    "            # Check if we're on the right page\n",
    "            if \"recent-activity\" not in self.driver.current_url:\n",
    "                logger.warning(f\"Not on activity page. Current URL: {self.driver.current_url}\")\n",
    "                if self.debug:\n",
    "                    self.driver.save_screenshot(f'debug/{self.session_id}_wrong_page.png')\n",
    "                return False\n",
    "            \n",
    "            # Wait for content to load\n",
    "            try:\n",
    "                # Wait for any of these elements that indicate posts are loaded\n",
    "                selectors = [\n",
    "                    \".occludable-update\",\n",
    "                    \".feed-shared-update-v2\",\n",
    "                    \".profile-creator-shared-feed-update__container\"\n",
    "                ]\n",
    "                \n",
    "                for selector in selectors:\n",
    "                    try:\n",
    "                        self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))\n",
    "                        logger.info(f\"Found posts with selector: {selector}\")\n",
    "                        return True\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                logger.warning(\"Could not find any post elements\")\n",
    "                if self.debug:\n",
    "                    self.driver.save_screenshot(f'debug/{self.session_id}_no_posts_found.png')\n",
    "                return False\n",
    "                \n",
    "            except TimeoutException:\n",
    "                logger.warning(\"Timeout waiting for posts to load\")\n",
    "                if self.debug:\n",
    "                    self.driver.save_screenshot(f'debug/{self.session_id}_posts_timeout.png')\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error navigating to profile: {str(e)}\")\n",
    "            if self.debug:\n",
    "                self.driver.save_screenshot(f'debug/{self.session_id}_navigation_error.png')\n",
    "            return False\n",
    "    \n",
    "    def scroll_to_top(self):\n",
    "        \"\"\"Scroll to the top of the page.\"\"\"\n",
    "        try:\n",
    "            self.driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "            time.sleep(2)\n",
    "            logger.info(\"Scrolled to top of page\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scrolling to top: {str(e)}\")\n",
    "            return False\n",
    "        \n",
    "    def check_for_redirect(self, original_url):\n",
    "        \"\"\"Check if LinkedIn has redirected us to a different page.\"\"\"\n",
    "        try:\n",
    "            current_url = self.driver.current_url\n",
    "            \n",
    "            # Extract the core profile identifier from both URLs\n",
    "            def extract_profile_id(url):\n",
    "                if '/in/' in url:\n",
    "                    # Extract profile slug from URL like /in/profile-name/\n",
    "                    profile_part = url.split('/in/')[1].split('/')[0]\n",
    "                    return profile_part\n",
    "                return None\n",
    "            \n",
    "            original_profile = extract_profile_id(original_url)\n",
    "            current_profile = extract_profile_id(current_url)\n",
    "            \n",
    "            # Check if we're still on the same profile\n",
    "            if original_profile and current_profile:\n",
    "                if original_profile != current_profile:\n",
    "                    logger.warning(f\"Profile redirect detected: {original_profile} -> {current_profile}\")\n",
    "                    return True\n",
    "            \n",
    "            # Check for common LinkedIn redirect patterns\n",
    "            redirect_indicators = [\n",
    "                '/feed/',\n",
    "                '/search/',\n",
    "                '/company/',\n",
    "                '/school/',\n",
    "                '/checkpoint/',\n",
    "                '/uas/login',\n",
    "                '/authwall',\n",
    "                '/login',\n",
    "                'linkedin.com/404',\n",
    "                'linkedin.com/error'\n",
    "            ]\n",
    "            \n",
    "            for indicator in redirect_indicators:\n",
    "                if indicator in current_url.lower():\n",
    "                    logger.warning(f\"LinkedIn redirect detected to: {current_url}\")\n",
    "                    return True\n",
    "            \n",
    "            # Check if we're no longer on a recent-activity page when we should be\n",
    "            if 'recent-activity' in original_url and 'recent-activity' not in current_url:\n",
    "                logger.warning(f\"Redirected away from activity page: {current_url}\")\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking for redirect: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def scroll_and_extract_incrementally(self, category, original_url, max_scrolls=15):\n",
    "        \"\"\"Scroll the page and extract posts incrementally. Stop if redirected but keep accumulated posts.\"\"\"\n",
    "        try:\n",
    "            # First scroll to top\n",
    "            self.scroll_to_top()\n",
    "            \n",
    "            # Take screenshot before scrolling\n",
    "            if self.debug:\n",
    "                self.driver.save_screenshot(f'debug/{self.session_id}_before_scrolling.png')\n",
    "            \n",
    "            posts_loaded = set()  # Track unique posts to avoid counting duplicates\n",
    "            self.accumulated_posts = []  # Reset accumulated posts for this profile\n",
    "            processed_texts = set()  # Track processed posts to avoid duplicates\n",
    "            \n",
    "            # Get profile name once\n",
    "            profile_name = self.extract_profile_name()\n",
    "            \n",
    "            # Scroll down gradually to load posts\n",
    "            for i in range(max_scrolls):\n",
    "                logger.info(f\"Scroll {i+1}/{max_scrolls}\")\n",
    "                \n",
    "                # Check for redirect before continuing\n",
    "                if self.check_for_redirect(original_url):\n",
    "                    logger.warning(f\"Redirect detected during scroll {i+1}\")\n",
    "                    logger.info(f\"Saving {len(self.accumulated_posts)} posts collected so far\")\n",
    "                    if self.debug:\n",
    "                        self.driver.save_screenshot(f'debug/{self.session_id}_redirect_detected.png')\n",
    "                    return self.accumulated_posts  # Return what we have so far\n",
    "                \n",
    "                # Find all \"see more\" links and expand them\n",
    "                self.expand_all_see_more()\n",
    "                \n",
    "                # Extract posts from current view\n",
    "                current_batch_posts = self.extract_current_posts(category, profile_name, processed_texts)\n",
    "                \n",
    "                # Add new posts to accumulated posts\n",
    "                new_posts_count = 0\n",
    "                for post in current_batch_posts:\n",
    "                    if post['post_text'] not in processed_texts:\n",
    "                        self.accumulated_posts.append(post)\n",
    "                        processed_texts.add(post['post_text'])\n",
    "                        new_posts_count += 1\n",
    "                \n",
    "                logger.info(f\"Found {new_posts_count} new posts. Total accumulated: {len(self.accumulated_posts)}\")\n",
    "                \n",
    "                # If we have enough posts, we can stop scrolling\n",
    "                if len(self.accumulated_posts) >= self.max_posts:\n",
    "                    logger.info(f\"Reached target of {self.max_posts} posts\")\n",
    "                    break\n",
    "                \n",
    "                # Scroll down more aggressively for more posts\n",
    "                self.driver.execute_script(\"window.scrollBy(0, 800);\")\n",
    "                time.sleep(3)\n",
    "                \n",
    "                # Check for redirect after scrolling\n",
    "                if self.check_for_redirect(original_url):\n",
    "                    logger.warning(f\"Redirect detected after scroll {i+1}\")\n",
    "                    logger.info(f\"Saving {len(self.accumulated_posts)} posts collected so far\")\n",
    "                    if self.debug:\n",
    "                        self.driver.save_screenshot(f'debug/{self.session_id}_redirect_after_scroll.png')\n",
    "                    return self.accumulated_posts  # Return what we have so far\n",
    "                \n",
    "                # Every 3 scrolls, take a screenshot and check URL\n",
    "                if self.debug and i % 3 == 0:\n",
    "                    self.driver.save_screenshot(f'debug/{self.session_id}_scrolling_{i+1}.png')\n",
    "            \n",
    "            # Final expansion of \"see more\" links\n",
    "            self.expand_all_see_more()\n",
    "            \n",
    "            # Final redirect check\n",
    "            if self.check_for_redirect(original_url):\n",
    "                logger.warning(\"Redirect detected during final expansion\")\n",
    "                logger.info(f\"Saving {len(self.accumulated_posts)} posts collected so far\")\n",
    "                return self.accumulated_posts\n",
    "            \n",
    "            # Final extraction\n",
    "            final_batch_posts = self.extract_current_posts(category, profile_name, processed_texts)\n",
    "            for post in final_batch_posts:\n",
    "                if post['post_text'] not in processed_texts:\n",
    "                    self.accumulated_posts.append(post)\n",
    "                    processed_texts.add(post['post_text'])\n",
    "            \n",
    "            # Take screenshot after scrolling\n",
    "            if self.debug:\n",
    "                self.driver.save_screenshot(f'debug/{self.session_id}_after_scrolling.png')\n",
    "            \n",
    "            logger.info(f\"Successfully completed scraping. Total posts: {len(self.accumulated_posts)}\")\n",
    "            return self.accumulated_posts\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during scrolling: {str(e)}\")\n",
    "            if self.debug:\n",
    "                self.driver.save_screenshot(f'debug/{self.session_id}_scrolling_error.png')\n",
    "            # Even on error, return what we have accumulated\n",
    "            logger.info(f\"Returning {len(self.accumulated_posts)} posts despite error\")\n",
    "            return self.accumulated_posts\n",
    "    \n",
    "    def extract_current_posts(self, category, profile_name, processed_texts):\n",
    "        \"\"\"Extract posts currently visible on the page.\"\"\"\n",
    "        try:\n",
    "            # Find all post containers\n",
    "            post_selectors = [\n",
    "                \".feed-shared-update-v2\",\n",
    "                \".occludable-update\",\n",
    "                \".profile-creator-shared-feed-update__container\"\n",
    "            ]\n",
    "            \n",
    "            all_posts = []\n",
    "            for selector in post_selectors:\n",
    "                posts = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                if posts:\n",
    "                    all_posts = posts\n",
    "                    break\n",
    "            \n",
    "            if not all_posts:\n",
    "                return []\n",
    "            \n",
    "            # Extract data from each post\n",
    "            post_data = []\n",
    "            \n",
    "            for i, post in enumerate(all_posts):\n",
    "                try:\n",
    "                    # Check if this is an original post\n",
    "                    if not self.is_original_post(post):\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract post data\n",
    "                    post_text = self.extract_post_text(post)\n",
    "                    \n",
    "                    # Skip if we've already processed this text or it's too short\n",
    "                    if post_text in processed_texts or len(post_text.strip()) < 10:\n",
    "                        continue\n",
    "                    \n",
    "                    # Add to post data\n",
    "                    post_data.append({\n",
    "                        'profile_name': profile_name,\n",
    "                        'post_text': post_text,\n",
    "                        'category': category\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing post {i+1}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            return post_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting current posts: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def count_loaded_posts(self):\n",
    "        \"\"\"Count the number of posts currently loaded on the page.\"\"\"\n",
    "        try:\n",
    "            post_selectors = [\n",
    "                \".feed-shared-update-v2\",\n",
    "                \".occludable-update\",\n",
    "                \".profile-creator-shared-feed-update__container\"\n",
    "            ]\n",
    "            \n",
    "            max_count = 0\n",
    "            for selector in post_selectors:\n",
    "                posts = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                max_count = max(max_count, len(posts))\n",
    "            \n",
    "            return max_count\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def expand_all_see_more(self):\n",
    "        \"\"\"Find and click all 'see more' links on the page.\"\"\"\n",
    "        try:\n",
    "            # Find all elements that might be \"see more\" buttons\n",
    "            see_more_selectors = [\n",
    "                \".inline-show-more-text__button\",\n",
    "                \".feed-shared-inline-show-more-text__see-more\",\n",
    "                \".feed-shared-text-view__see-more\",\n",
    "                \".see-more\",\n",
    "                \"span.lt-line-clamp__more\"\n",
    "            ]\n",
    "            \n",
    "            for selector in see_more_selectors:\n",
    "                try:\n",
    "                    see_more_buttons = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    logger.info(f\"Found {len(see_more_buttons)} potential 'see more' buttons with selector: {selector}\")\n",
    "                    \n",
    "                    for button in see_more_buttons:\n",
    "                        try:\n",
    "                            if button.is_displayed():\n",
    "                                # Try to scroll to the button\n",
    "                                self.driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});\", button)\n",
    "                                time.sleep(1)\n",
    "                                \n",
    "                                # Try JavaScript click\n",
    "                                try:\n",
    "                                    self.driver.execute_script(\"arguments[0].click();\", button)\n",
    "                                    logger.info(\"Expanded post with JS click\")\n",
    "                                    time.sleep(1)\n",
    "                                except:\n",
    "                                    # Try regular click\n",
    "                                    try:\n",
    "                                        button.click()\n",
    "                                        logger.info(\"Expanded post with regular click\")\n",
    "                                        time.sleep(1)\n",
    "                                    except:\n",
    "                                        pass\n",
    "                        except:\n",
    "                            continue\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Also try a more aggressive approach with JavaScript\n",
    "            try:\n",
    "                expanded_count = self.driver.execute_script(\"\"\"\n",
    "                    const expandButtons = [];\n",
    "                    \n",
    "                    // Find all elements containing \"...more\" or \"see more\" text\n",
    "                    const allElements = document.querySelectorAll('*');\n",
    "                    for (const el of allElements) {\n",
    "                        const text = el.textContent;\n",
    "                        if ((text.includes('…more') || \n",
    "                             text.includes('...more') || \n",
    "                             text.toLowerCase().includes('see more')) && \n",
    "                            el.offsetWidth > 0 && \n",
    "                            el.offsetHeight > 0) {\n",
    "                            \n",
    "                            try {\n",
    "                                el.click();\n",
    "                                expandButtons.push(el);\n",
    "                            } catch (e) {\n",
    "                                // Try parent element\n",
    "                                try {\n",
    "                                    el.parentElement.click();\n",
    "                                    expandButtons.push(el.parentElement);\n",
    "                                } catch (e2) {\n",
    "                                    // Ignore\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    return expandButtons.length;\n",
    "                \"\"\")\n",
    "                \n",
    "                if expanded_count > 0:\n",
    "                    logger.info(f\"Expanded {expanded_count} 'see more' buttons with JavaScript\")\n",
    "                    time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error expanding 'see more' links: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_posts(self, category):\n",
    "        \"\"\"Extract all original posts from the current page. Modified to handle 50 posts.\"\"\"\n",
    "        try:\n",
    "            if self.debug:\n",
    "                self.driver.save_screenshot(f'debug/{self.session_id}_before_extraction.png')\n",
    "            \n",
    "            # First, scroll to top to ensure we start from the top (most recent posts)\n",
    "            self.scroll_to_top()\n",
    "            time.sleep(3)  # Give page time to load top content\n",
    "            \n",
    "            # Find all post containers - increased limit to get more posts\n",
    "            post_selectors = [\n",
    "                \".feed-shared-update-v2\",\n",
    "                \".occludable-update\",\n",
    "                \".profile-creator-shared-feed-update__container\"\n",
    "            ]\n",
    "            \n",
    "            all_posts = []\n",
    "            for selector in post_selectors:\n",
    "                posts = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                if posts:\n",
    "                    logger.info(f\"Found {len(posts)} posts with selector: {selector}\")\n",
    "                    # Increased from 15 to handle more posts, but cap at reasonable number\n",
    "                    all_posts = posts[:100]  # Take up to 100 posts to filter from\n",
    "                    break\n",
    "            \n",
    "            if not all_posts:\n",
    "                logger.warning(\"No posts found\")\n",
    "                if self.debug:\n",
    "                    self.driver.save_screenshot(f'debug/{self.session_id}_no_posts.png')\n",
    "                return []\n",
    "            \n",
    "            # Extract data from each post\n",
    "            post_data = []\n",
    "            post_count = 0\n",
    "            processed_texts = set()  # Track processed posts to avoid duplicates\n",
    "            \n",
    "            for i, post in enumerate(all_posts):\n",
    "                try:\n",
    "                    # Stop if we've reached our target\n",
    "                    if post_count >= self.max_posts:\n",
    "                        logger.info(f\"Reached target of {self.max_posts} posts\")\n",
    "                        break\n",
    "                    \n",
    "                    logger.info(f\"Processing post {i+1}/{len(all_posts)} (extracted: {post_count})\")\n",
    "                    \n",
    "                    # Scroll to the post to ensure it's in view\n",
    "                    try:\n",
    "                        self.driver.execute_script(\"arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});\", post)\n",
    "                        time.sleep(1)\n",
    "                    except:\n",
    "                        logger.warning(f\"Could not scroll to post {i+1}\")\n",
    "                    \n",
    "                    # Debug screenshot every 10 posts\n",
    "                    if self.debug and i % 10 == 0:\n",
    "                        self.driver.save_screenshot(f'debug/{self.session_id}_post_{i+1}.png')\n",
    "                    \n",
    "                    # Check if this is an original post\n",
    "                    if not self.is_original_post(post):\n",
    "                        logger.info(f\"Post {i+1} is not an original post, skipping\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract post data\n",
    "                    post_text = self.extract_post_text(post)\n",
    "                    \n",
    "                    # Skip if we've already processed this text (duplicate detection)\n",
    "                    if post_text in processed_texts or len(post_text.strip()) < 10:\n",
    "                        logger.info(f\"Post {i+1} is duplicate or too short, skipping\")\n",
    "                        continue\n",
    "                    \n",
    "                    processed_texts.add(post_text)\n",
    "                    \n",
    "                    # Log the post data being extracted\n",
    "                    logger.info(f\"Post {i+1}: Text length={len(post_text)}\")\n",
    "                    \n",
    "                    # Get profile name\n",
    "                    profile_name = self.extract_profile_name()\n",
    "                    \n",
    "                    # Add to post data - only the required fields\n",
    "                    post_data.append({\n",
    "                        'profile_name': profile_name,\n",
    "                        'post_text': post_text,\n",
    "                        'category': category\n",
    "                    })\n",
    "                    \n",
    "                    logger.info(f\"Successfully extracted post {post_count + 1}\")\n",
    "                    \n",
    "                    # Increment post count\n",
    "                    post_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing post {i+1}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"Extracted {len(post_data)} posts\")\n",
    "            return post_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting posts: {str(e)}\")\n",
    "            if self.debug:\n",
    "                self.driver.save_screenshot(f'debug/{self.session_id}_extraction_error.png')\n",
    "            return []\n",
    "    \n",
    "    def is_original_post(self, post):\n",
    "        \"\"\"Check if a post is an original post (not a like, comment, etc.).\"\"\"\n",
    "        try:\n",
    "            # Check for activity indicators\n",
    "            activity_texts = [\n",
    "                \"liked\", \"commented on\", \"replied\", \"reposted\", \n",
    "                \"shared\", \"celebrates\", \"mentioned in\", \"follows\"\n",
    "            ]\n",
    "            \n",
    "            post_text = post.text.lower()\n",
    "            \n",
    "            # If the post contains any activity indicators at the beginning, it's not original\n",
    "            for activity in activity_texts:\n",
    "                if post_text.startswith(activity) or f\"\\n{activity}\" in post_text[:50]:\n",
    "                    return False\n",
    "            \n",
    "            # Check for content indicators\n",
    "            content_selectors = [\n",
    "                \".feed-shared-update-v2__description\",\n",
    "                \".feed-shared-text\",\n",
    "                \".update-components-text\",\n",
    "                \".feed-shared-text-view\",\n",
    "                \".update-components-update-v2__commentary\"\n",
    "            ]\n",
    "            \n",
    "            for selector in content_selectors:\n",
    "                content_elements = post.find_elements(By.CSS_SELECTOR, selector)\n",
    "                if content_elements and any(el.text.strip() for el in content_elements):\n",
    "                    return True\n",
    "            \n",
    "            # If we can't determine, assume it's not original\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking if post is original: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_post_text(self, post):\n",
    "        \"\"\"Extract the text content of a post.\"\"\"\n",
    "        try:\n",
    "            # Try to expand \"see more\" links in this post\n",
    "            self.expand_see_more_in_post(post)\n",
    "            \n",
    "            # Try different selectors for post content\n",
    "            content_selectors = [\n",
    "                \".feed-shared-update-v2__description\",\n",
    "                \".feed-shared-text\",\n",
    "                \".update-components-text\",\n",
    "                \".feed-shared-text-view\"\n",
    "            ]\n",
    "            \n",
    "            post_text = \"\"\n",
    "            for selector in content_selectors:\n",
    "                elements = post.find_elements(By.CSS_SELECTOR, selector)\n",
    "                for element in elements:\n",
    "                    text = element.text.strip()\n",
    "                    if text and len(text) > len(post_text):\n",
    "                        post_text = text\n",
    "            \n",
    "            # If no text found, try JavaScript\n",
    "            if not post_text:\n",
    "                post_text = self.driver.execute_script(\"\"\"\n",
    "                    const post = arguments[0];\n",
    "                    \n",
    "                    // Try to find the main text content\n",
    "                    const contentElements = post.querySelectorAll('p, span.break-words, div.break-words');\n",
    "                    let text = '';\n",
    "                    \n",
    "                    for (const el of contentElements) {\n",
    "                        if (el.textContent.trim() && el.offsetWidth > 0 && el.offsetHeight > 0) {\n",
    "                            text += el.textContent.trim() + '\\\\n';\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    return text.trim();\n",
    "                \"\"\", post)\n",
    "            \n",
    "            # Clean up the text\n",
    "            post_text = re.sub(r'\\n\\s*\\n', '\\n\\n', post_text)  # Remove extra newlines\n",
    "            post_text = re.sub(r' +', ' ', post_text)  # Remove extra spaces\n",
    "            \n",
    "            return post_text.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting post text: {str(e)}\")\n",
    "            return \"Error extracting text\"\n",
    "    \n",
    "    def expand_see_more_in_post(self, post):\n",
    "        \"\"\"Expand 'see more' links in a specific post.\"\"\"\n",
    "        try:\n",
    "            # Find all \"see more\" links in this post\n",
    "            see_more_selectors = [\n",
    "                \".inline-show-more-text__button\",\n",
    "                \".feed-shared-inline-show-more-text__see-more\",\n",
    "                \".feed-shared-text-view__see-more\",\n",
    "                \".see-more\",\n",
    "                \"span.lt-line-clamp__more\"\n",
    "            ]\n",
    "            \n",
    "            for selector in see_more_selectors:\n",
    "                try:\n",
    "                    see_more_buttons = post.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for button in see_more_buttons:\n",
    "                        try:\n",
    "                            if button.is_displayed():\n",
    "                                # Try JavaScript click\n",
    "                                self.driver.execute_script(\"arguments[0].click();\", button)\n",
    "                                time.sleep(1)\n",
    "                        except:\n",
    "                            pass\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Also try with JavaScript\n",
    "            self.driver.execute_script(\"\"\"\n",
    "                const post = arguments[0];\n",
    "                \n",
    "                // Find all elements containing \"...more\" or \"see more\" text\n",
    "                const allElements = post.querySelectorAll('*');\n",
    "                for (const el of allElements) {\n",
    "                    const text = el.textContent;\n",
    "                    if ((text.includes('…more') || \n",
    "                         text.includes('...more') || \n",
    "                         text.toLowerCase().includes('see more')) && \n",
    "                        el.offsetWidth > 0 && \n",
    "                        el.offsetHeight > 0) {\n",
    "                        \n",
    "                        try {\n",
    "                            el.click();\n",
    "                        } catch (e) {\n",
    "                            // Try parent element\n",
    "                            try {\n",
    "                                el.parentElement.click();\n",
    "                            } catch (e2) {\n",
    "                                // Ignore\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            \"\"\", post)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error expanding 'see more' in post: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_profile_name(self):\n",
    "        \"\"\"Extract the profile name from the current page.\"\"\"\n",
    "        try:\n",
    "            # Try to find the profile name\n",
    "            profile_name = self.driver.execute_script(\"\"\"\n",
    "                // Try to find profile name\n",
    "                const nameElement = document.querySelector('h1.text-heading-xlarge') || \n",
    "                                   document.querySelector('.pv-text-details__left-panel h1');\n",
    "                return nameElement ? nameElement.textContent.trim() : null;\n",
    "            \"\"\")\n",
    "            \n",
    "            if not profile_name:\n",
    "                # Try to extract from URL\n",
    "                current_url = self.driver.current_url\n",
    "                if '/in/' in current_url:\n",
    "                    profile_name = current_url.split('/in/')[1].split('/')[0].replace('-', ' ').title()\n",
    "                else:\n",
    "                    profile_name = \"Unknown Profile\"\n",
    "            \n",
    "            return profile_name\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting profile name: {str(e)}\")\n",
    "            return \"Unknown Profile\"\n",
    "    \n",
    "    def scrape_profile(self, profile_url, category):\n",
    "        \"\"\"Scrape a LinkedIn profile for original posts. Save posts incrementally even if redirected.\"\"\"\n",
    "        try:\n",
    "            # Navigate to the profile\n",
    "            if not self.navigate_to_profile(profile_url):\n",
    "                logger.error(f\"Failed to navigate to profile: {profile_url}\")\n",
    "                return []\n",
    "            original_url = profile_url\n",
    "            \n",
    "            # Use the incremental scrolling method that handles redirects\n",
    "            posts = self.scroll_and_extract_incrementally(category, original_url)\n",
    "            \n",
    "            # Save posts immediately, even if we were redirected\n",
    "            if posts:\n",
    "                self.save_posts_to_csv(posts, category)\n",
    "                logger.info(f\"Successfully saved {len(posts)} posts from profile\")\n",
    "            else:\n",
    "                logger.warning(\"No posts were extracted from the profile\")\n",
    "            \n",
    "            return posts\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping profile {profile_url}: {str(e)}\")\n",
    "            if self.debug:\n",
    "                self.driver.save_screenshot(f'debug/{self.session_id}_scrape_profile_error.png')\n",
    "            \n",
    "            # Even on error, try to save any accumulated posts\n",
    "            if hasattr(self, 'accumulated_posts') and self.accumulated_posts:\n",
    "                logger.info(f\"Saving {len(self.accumulated_posts)} posts despite error\")\n",
    "                self.save_posts_to_csv(self.accumulated_posts, category)\n",
    "                return self.accumulated_posts\n",
    "            \n",
    "            return []\n",
    "    \n",
    "    def save_posts_to_csv(self, posts, category):\n",
    "        \"\"\"Save posts to a CSV file.\"\"\"\n",
    "        try:\n",
    "            if not posts:\n",
    "                logger.warning(\"No posts to save\")\n",
    "                return False\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(posts)\n",
    "            \n",
    "            # Create filename with timestamp\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"data/linkedin_posts_{category}_{timestamp}.csv\"\n",
    "            \n",
    "            # Save to CSV\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            logger.info(f\"Saved {len(posts)} posts to {filename}\")\n",
    "            \n",
    "            # Also save a backup with session ID\n",
    "            backup_filename = f\"data/linkedin_posts_{category}_{self.session_id}.csv\"\n",
    "            df.to_csv(backup_filename, index=False, encoding='utf-8')\n",
    "            logger.info(f\"Backup saved to {backup_filename}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving posts to CSV: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the browser and clean up.\"\"\"\n",
    "        try:\n",
    "            if hasattr(self, 'driver'):\n",
    "                self.driver.quit()\n",
    "                logger.info(\"Browser closed successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error closing browser: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the scraper.\"\"\"\n",
    "    MAX_POSTS_PER_PROFILE = 20  # Maximum posts to scrape per profile\n",
    "    HEADLESS = False            # Set to True to run without browser window\n",
    "    DEBUG = True                # Set to False to disable debug screenshots\n",
    "\n",
    "    # Initialize scraper\n",
    "    scraper = LinkedInScraper(\n",
    "        headless=HEADLESS,\n",
    "        debug=DEBUG,\n",
    "        max_posts=MAX_POSTS_PER_PROFILE\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(\"build.csv\")\n",
    "\n",
    "        # Prepare list to collect all posts\n",
    "        all_posts = []\n",
    "\n",
    "        for idx, row in data.iterrows():\n",
    "            profile_url = row[\"LinkedIn_URL\"]\n",
    "            category = row[\"Cat\"]\n",
    "\n",
    "            posts = scraper.scrape_profile(profile_url, category)\n",
    "            \n",
    "            # Assume each post is a dictionary\n",
    "            for post in posts:\n",
    "                post[\"profile_url\"] = profile_url\n",
    "                post[\"category\"] = category\n",
    "                all_posts.append(post)\n",
    "\n",
    "        # Save all collected posts to a new CSV file\n",
    "        if all_posts:\n",
    "            df_posts = pd.DataFrame(all_posts)\n",
    "            df_posts.to_csv(\"scraped_posts.csv\", index=False)\n",
    "            logger.info(f\"Saved {len(all_posts)} posts to scraped_posts.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during scraping: {str(e)}\")\n",
    "\n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589de3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KNPRO\\Desktop\\projects\\linkedin-app-complete\\langgraph_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-20 13:30:25,231 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-20 13:30:25,233 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "2025-06-20 13:30:29,989 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New ChromaDB created and populated.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# === Step 1: Load CSV ===\n",
    "df = pd.read_csv(\"scraped_posts.csv\")  # Replace with actual path\n",
    "assert {'profile_name','post_text','category','profile_url'}.issubset(df.columns)\n",
    "\n",
    "# === Step 2: Embed Text ===\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(df['post_text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# === Step 3: Initialize Fresh ChromaDB ===\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")  # This is now a new DB\n",
    "collection = client.get_collection(name=\"posts_collection\")\n",
    "\n",
    "# === Step 4: Insert Data ===\n",
    "collection.add(\n",
    "    documents=df['post_text'].tolist(),\n",
    "    embeddings=embeddings,\n",
    "    ids=[f\"post_{i}\" for i in range(len(df))],\n",
    "    metadatas=[\n",
    "        {\n",
    "            \"profile_name\": row[\"profile_name\"],\n",
    "            \"category\": row[\"category\"]\n",
    "        } for _, row in df.iterrows()\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"✅ New ChromaDB created and populated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e17473ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: post_0\n",
      "Post Text: Curious to know what 1 day of ChatGPT costs OpenAI in terms of energy (in ovens 🔥) and water (in bathtubs 🛀)?\n",
      "\n",
      "1 day of ChatGPT = ~1bn queries\n",
      "That's roughly: \n",
      "💧🛀 1000 bathtubs of water\n",
      "⚡🔥 6000 ovens running for a day\n",
      "\n",
      "Here's Sam Altman's original quote:\n",
      "\n",
      "\"The average query uses about 0.34 watt-hours, about what an oven would use in a little over one second, or a high-efficiency lightbulb would use in a couple of minutes. It also uses about 0.000085 gallons of water; roughly one fifteenth of a teaspoon.\"\n",
      "Metadata: {'profile_name': 'cassie-kozyrkov-9531919', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_1\n",
      "Post Text: There are 2 kinds of AI-first memos:\n",
      "1. The kind CEOs issue to sound bold and visionary.\n",
      "2. The kind Cassie cassie-kozyrkov-9531919 sent to her team.\n",
      "\n",
      "We saw Shopify, Duolingo, Fiverr and others try the first way.\n",
      "Didn’t exactly go over well.\n",
      "\n",
      "But Cassie’s version sets the gold standard.\n",
      "\n",
      "Not just for “AI-firstitude” as she calls it.\n",
      "But for leadership in a time of uncertainty.\n",
      "(which we are most certainly in)\n",
      "\n",
      "Her message is thoughtful, empowering, and deeply human.\n",
      "\n",
      "It doesn’t command. It guides. \n",
      "It doesn’t hype. It earns trust.\n",
      "\n",
      "“Please cheat at your job.”\n",
      "That’s how she opens.\n",
      "\n",
      "But the message isn’t about cutting corners. \n",
      "It’s about using judgment. \n",
      "Embracing tools. \n",
      "Asking smart questions. \n",
      "Leading with curiosity, not fear.\n",
      "\n",
      "“Using a generative AI tool is very easy… but using it well takes smarts and experience.”\n",
      "\n",
      "“Don’t assume AI is intelligent… You bring the intelligence.”\n",
      "\n",
      "“Start by figuring out what’s worth doing, then ask an AI system like ChatGPT how to do it better.”\n",
      "\n",
      "No ultimatums. No bravado. \n",
      "Just clarity, humility, and direction.\n",
      "\n",
      "As a marketer, I admire this as a masterclass in internal communication. \n",
      "As someone navigating the AI era, I admire it even more as leadership.\n",
      "\n",
      "If you’re writing an AI-first memo to your team,\n",
      "or just trying to get them aligned on what’s next...\n",
      "\n",
      "Steal this.\n",
      "Share it.\n",
      "Encourage others to pass it on.\n",
      "\n",
      "(Link to full memo in comments)\n",
      "\n",
      "-----\n",
      "\n",
      "Follow me for AI news, just for marketers. 🔔\n",
      "Share this story with your people. ♻️ \n",
      "Subscribe for free at JoinAIMarketer(dot)com. ☠️\n",
      "\n",
      "hashtag\n",
      "#AIFirst \n",
      "hashtag\n",
      "#AIForward \n",
      "hashtag\n",
      "#AIMarketing\n",
      "Metadata: {'category': 'AI', 'profile_name': 'cassie-kozyrkov-9531919'}\n",
      "----------------------------------------\n",
      "ID: post_2\n",
      "Post Text: I’ve long said the best analogy for \n",
      "hashtag\n",
      "#AI—especially general purpose or generative AI—is a magic lamp with a genie inside. (Years before the \n",
      "hashtag\n",
      "#GenAI pun, you’re welcome.)\n",
      "\n",
      "Here’s the thing:\n",
      "For consumers, it doesn’t matter much. But for businesses? Incredibly attractive. Selling a genie now that can grant unknown wishes later is a goldmine. That’s why Silicon Valley races to build the ultimate 🧞‍♂️ genie—whether it’s GenAI or AGI, it’s all “wish potential” if you squint at it.\n",
      "\n",
      "But with agentic AI, we need to talk about the lamp. 🪔 The lamp is the control layer—guardrails that keep powerful genies in check. Unlike chatbots, which simply supply information for you to use as you please (or not), agentic AI acts on the world. Poor lamp design? Real-world harm. \n",
      "\n",
      "But worse than a strong genie or a weak lamp? An unskilled wisher. 🤦 \n",
      "\n",
      "AI needs oversight. It’s a power tool—amplifying whatever judgment it’s handed.\n",
      "\n",
      "Over dinner last night at the brilliant 𝐓𝐞𝐜𝐡𝐧𝐨𝐥𝐨𝐠𝐲 𝐋𝐞𝐚𝐝𝐞𝐫’𝐬 𝐓𝐚𝐛𝐥𝐞 (thanks to The Future Solving Company & IBM for hosting us), we explored what this means:\n",
      "✨ Prompting the genie wisely and precisely\n",
      "✨ Anticipating ripple effects\n",
      "✨ Embedding foresight into the wish\n",
      "✨ Owning by the outcome—because the further the AI acts, the harder it is to predict\n",
      "\n",
      "Harder doesn’t mean give up. It means level up.\n",
      "\n",
      "Here’s the question I asked the table—and now you:\n",
      "\"If a real magic lamp landed in your hands, what would you regret not having learned first? What tools or skills would you lament not already having?\"\n",
      "\n",
      "Answer that before it's time to wish. Because powerful general purpose tech that plays at the organization scale is coming. And I want you ready.\n",
      "\n",
      "Grateful to the extraordinary leaders at the table—Ann Leach, Matt Sanchez, Tom Donaldson, Amitabh Apte,Ngozi Nwosu-Aligwekwe FRSA, Petros Rizos, David Cushman, Robert De La Rue, Jad Freiha, Dr. Wendy Ng, CISSP, Jennifer Heape, Alicia Teltz. (Based on their great answers, there’s hope for a new breed of leaders who will step up.)\n",
      "\n",
      "Now it’s your turn—drop a comment:\n",
      "\n",
      "👑 What’s one thing you’d regret not knowing when your digital genie shows up? 👑\n",
      "\n",
      "Feeling dinner salon FOMO or want to cohost this in your city? \n",
      "👉 https://lnkd.in/e9bACyQz\n",
      "\n",
      "📸: Mitzi for Raccoon London\n",
      "Metadata: {'profile_name': 'cassie-kozyrkov-9531919', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_3\n",
      "Post Text: ⚡ Lightning round! Can I answer 5 burning questions you have about AI agent management in under 2.5 minutes? (Challenge accepted!)\n",
      "\n",
      "Here they are:\n",
      "\n",
      "1) What the \n",
      "hashtag\n",
      "#agentic revolution is all about... and why is it human glue?\n",
      "2) What happens when tools meet the real world?\n",
      "3) Protocols in a nutshell: what are MCP and A2A?\n",
      "4) What's the AI Reliability Paradox and what should you do about it?\n",
      "5) Should you trust \n",
      "hashtag\n",
      "#AI \n",
      "hashtag\n",
      "#agents?\n",
      "\n",
      "All in the video below, brought to you in \n",
      "hashtag\n",
      "#partnership with @Boomi \n",
      "hashtag\n",
      "#Agentstudio. It was a true pleasure to speak at \n",
      "hashtag\n",
      "#BoomiWorld this year.\n",
      "\n",
      "So, how did I do on those questions? ;) \n",
      "\n",
      "Repost ♻️ if you think I did pretty well for 2:21min...\n",
      "Metadata: {'category': 'AI', 'profile_name': 'cassie-kozyrkov-9531919'}\n",
      "----------------------------------------\n",
      "ID: post_4\n",
      "Post Text: \"Don’t assume AI is intelligent. AI is just a tool, it’s not your boss. You bring the intelligence. So don’t follow AI’s advice blindly. Sometimes there’s no good advice for your situation, but it’s always worth asking. Especially when asking has never been cheaper. You’ve got nothing to lose… as long as you fact-check anything that you’re tempted to act on. \n",
      "\n",
      "Do not fall asleep at the wheel. You're responsible for the quality of your work, with or without AI. AI cannot take responsibility. Ever. You are responsible for what you do with your tools\".\n",
      "\n",
      "That is just a single paragraph form Cassie cassie-kozyrkov-9531919 fantastic blog for people who treat ChatGPT with more respect than human copywriters. \n",
      "\n",
      "Last week, I caught myself posting the same comment under yet another AI yay or nay-style post: “Crap in, crap out.”\n",
      "\n",
      "AI won’t revolutionise your marketing, or your business, if what comes out is bland, lazy, or lacks the necessary, emotional pull. \n",
      "\n",
      "We haven’t even seen the full fallout yet, but shortcutting our way through ChatGPT and other LLMs is already eroding trust and damaging reputations. \n",
      "\n",
      "And if you’re a small brand? That damage will cut deeper.\n",
      "\n",
      "If you’re drowning in AI noise, make Cassie's post the one you actually read. \n",
      "\n",
      "https://lnkd.in/ecTiEBqk\n",
      "Metadata: {'profile_name': 'cassie-kozyrkov-9531919', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_5\n",
      "Post Text: Which of these (human) workers is most dangerous (and why)?\n",
      "\n",
      "🤦 Chris Careless 🤦 is a constant disappointment to you, performing your task well 70% of the time and being a total cringe magnet the rest of the time. Watching Chris make 10 attempts is more than enough to provoke an “oh, dear” response from you.\n",
      "\n",
      "😎 Ronnie Reliable 😎 is another story. You’ve seen Ronnie in action over a hundred times and you’ve been consistently impressed.\n",
      "\n",
      "Here comes the billion-dollar question. Which worker is more dangerous to your business? 😎 or 🤦 ? Why? And which worker is more like AI in this analogy...?\n",
      "\n",
      "If you find yourself hesitating even a little on this answer, please read the full explanation here: https://lnkd.in/e4EH56qJ \n",
      "\n",
      "If you found this useful, a repost ♻️ makes my heart happy. And a subscription to my newsletter makes my day. decision.substack.com\n",
      "\n",
      "hashtag\n",
      "#ai \n",
      "hashtag\n",
      "#reliability \n",
      "hashtag\n",
      "#paradox\n",
      "Metadata: {'category': 'AI', 'profile_name': 'cassie-kozyrkov-9531919'}\n",
      "----------------------------------------\n",
      "ID: post_6\n",
      "Post Text: You don’t rise to the level of your system’s performance.\n",
      "You fall to the level of its failure modes.\n",
      "\n",
      "Let’s talk about the AI Reliability Paradox.\n",
      "\n",
      "When something works 99.99% of the time, what do most leaders do?\n",
      "They round it up to 100%.\n",
      "Then they trust it.\n",
      "Then it breaks.\n",
      "\n",
      "The more impressive your system is, the more likely you are to let your guard down—and the more painful the failure when it comes.\n",
      "\n",
      "As you start to deploy AI systems at scale, remember: high-performing AI can be more dangerous than mediocre systems... if you don't build safety nets.\n",
      "\n",
      "I wrote more about this very topic in my latest Substack.\n",
      "\n",
      "Read more here 👉 https://lnkd.in/e4EH56qJ \n",
      "\n",
      "If you found this useful, a repost ♻️ makes my heart happy. And a subscription to my newsletter makes my day. decision.substack.com\n",
      "\n",
      "hashtag\n",
      "#ai \n",
      "hashtag\n",
      "#reliability \n",
      "hashtag\n",
      "#paradox\n",
      "Metadata: {'profile_name': 'cassie-kozyrkov-9531919', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_7\n",
      "Post Text: What is the AI Reliability Paradox? Find out in the next installment of my newsletter, which is ready to pop into your inbox tomorrow morning! Subscribe at decision.substack.com to get it. ✨\n",
      "Metadata: {'profile_name': 'cassie-kozyrkov-9531919', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_8\n",
      "Post Text: 🇬🇧 My bags are packed: London, here I come!\n",
      "\n",
      "And yes, I've packed my love of AI, decision science, and philosophical dinner chatter with me.\n",
      "\n",
      "On 𝐓𝐡𝐮𝐫𝐬𝐝𝐚𝐲, 𝐉𝐮𝐧𝐞 𝟏𝟐, I’ll be speaking at a cozy-but-powerful gathering called the 𝐓𝐞𝐜𝐡𝐧𝐨𝐥𝐨𝐠𝐲 𝐋𝐞𝐚𝐝𝐞𝐫’𝐬 𝐓𝐚𝐛𝐥𝐞, hosted by the wonderful folks at The Future Solving Company and IBM .\n",
      "\n",
      "Our topic: 𝐀𝐈 𝐀𝐠𝐞𝐧𝐭𝐬.\n",
      "What they are, why they matter, and how they’re changing the way we think about intelligence, autonomy, and the future of leadership.\n",
      "\n",
      "Our format: A stunning venue, a beautifully set table, and real conversation (no panels, no pitches). The kind of setting that helps ideas breathe.\n",
      "\n",
      "If you’re a senior technology leader based in London and this sounds like your kind of evening, 𝐃𝐌 Brian Evergreen the words “𝐋𝐎𝐍𝐃𝐎𝐍 𝐃𝐈𝐍𝐍𝐄𝐑” and we’ll save you a seat. \n",
      "\n",
      "Not in London? Want to bring this to your city? Or curious about cohosting? Let us know here: https://lnkd.in/e9bACyQz\n",
      "\n",
      "Let’s talk about the future like grownups: with nuance, vision, and dessert.\n",
      "\n",
      "See you soon?\n",
      "Metadata: {'category': 'AI', 'profile_name': 'cassie-kozyrkov-9531919'}\n",
      "----------------------------------------\n",
      "ID: post_9\n",
      "Post Text: Remember: What you see from \n",
      "hashtag\n",
      "#AI today isn’t the ceiling — it’s the floor.\n",
      "\n",
      "The tech is moving fast. Faster than companies, careers, and culture can keep up.\n",
      "\n",
      "This gap won’t last forever — but it does require urgent attention. Because when it closes, it won’t be gradual. And the first to feel it will be those in entry-level roles.\n",
      "\n",
      "I wrote about what’s shifting, what’s at stake, and what we need to do now → https://lnkd.in/eNjwgiju \n",
      "\n",
      "If you find this useful, a repost ♻️ makes my heart happy.\n",
      "\n",
      "hashtag\n",
      "#careers \n",
      "hashtag\n",
      "#entrylevel \n",
      "hashtag\n",
      "#automation\n",
      "Metadata: {'category': 'AI', 'profile_name': 'cassie-kozyrkov-9531919'}\n",
      "----------------------------------------\n",
      "ID: post_10\n",
      "Post Text: 💊 The \n",
      "hashtag\n",
      "#FDA is rolling out \n",
      "hashtag\n",
      "#Elsa, a \n",
      "hashtag\n",
      "#GenAI tool now going agency-wide by June 30 of this year. This is huge news!\n",
      "\n",
      "Developing a new drug, from its initial discovery to final FDA approval, typically spans 10 to 12 years. A significant portion of this time, often 6 to 7 years, is spent on the clinical trial and regulatory review phases.\n",
      "\n",
      "FDA inefficiencies can delay drug approvals by 1 to 3+ years, driven by slow reviews, unclear guidance, and bureaucratic hurdles—costing manufacturers valuable time, revenue, and market opportunity. For blockbuster drugs, each day of delay can mean $1–10 million in lost revenue, making regulatory slowdowns a major financial burden.\n",
      "\n",
      "In other words, even if you've invented a cure for something, your cure will spend years in purgatory. This move by the FDA aims to cut down that massive chunk of time.\n",
      "\n",
      "Think about what this means:\n",
      "\n",
      "💊 The time that effective new medicines will spend in testing purgatory will be decreased dramatically. This is huge for pharma and for consumers! 💊 \n",
      "\n",
      "Elsa is designed to streamline internal workflows—starting with clinical trial protocol reviews—which means we can expect faster, safer drug development and a more efficient regulatory process. \n",
      "\n",
      "This is a major milestone in federal \n",
      "hashtag\n",
      "#AI adoption that’s flying under the radar. It signals a shift in how public health agencies will operate going forward—leaner, smarter, and AI-assisted. One to watch closely.\n",
      "\n",
      "https://lnkd.in/eYvHVtUA\n",
      "Metadata: {'category': 'AI', 'profile_name': 'cassie-kozyrkov-9531919'}\n",
      "----------------------------------------\n",
      "ID: post_11\n",
      "Post Text: Inherited \n",
      "hashtag\n",
      "#datasets are like inherited toothbrushes: using them is an act of desperation. You’d always prefer to use your own if possible, since secondhand datasets are rife with gotchas. Unfortunately, you might not have that option...\n",
      "\n",
      "For a quick guide to working with inherited data, check out the video.\n",
      "\n",
      "For a deep dive, here's the full post: https://lnkd.in/etYG7pTp\n",
      "Metadata: {'profile_name': 'cassie-kozyrkov-9531919', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_12\n",
      "Post Text: I said it in 2023 and I'll say it again. I don't believe that \n",
      "hashtag\n",
      "#AI is the end of productivity. I believe it's the beginning. If I had to place my bets, they'd be on all of us having more to do in the AI era, not less. \n",
      "\n",
      "If we play our cards right, all that automation will free us up to pursue the grandest challenges of our times...\n",
      "Metadata: {'profile_name': 'cassie-kozyrkov-9531919', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_13\n",
      "Post Text: These two things can be true at once:\n",
      "\n",
      "1) AI is coming for jobs — especially entry-level roles built on repetitive, digitized tasks that anyone can do.\n",
      "2) I’m still optimistic about the future.\n",
      "\n",
      "But optimism doesn’t mean denial.\n",
      "\n",
      "I’m willing to bet that in the long run, AI will create plenty of work for us all to do, even as it assists that work, just like laptops and the internet created all the workers we’re now worrying about. But until then, the entry-level job market is about to get rocked.\n",
      "\n",
      "I wrote more about this this, and what we can do, over in my Substack → decision.substack.com\n",
      "Metadata: {'profile_name': 'cassie-kozyrkov-9531919', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_14\n",
      "Post Text: “Data is a memory prosthesis—it allows us to share our memories and to share them at scale.” 🤖✨\n",
      "\n",
      "Cassie cassie-kozyrkov-9531919, CEO of Kozyr, breaks down AI adoption beyond the buzzwords—revealing how bold strategies and real-world application are shaping the next era of travel and business.\n",
      "\n",
      "🔗 Register your interest: http://spkl.io/6045fARl3\n",
      "\n",
      "hashtag\n",
      "#ATMDubai \n",
      "hashtag\n",
      "#ATM2025 \n",
      "hashtag\n",
      "#TravelTech \n",
      "hashtag\n",
      "#AIinTravel \n",
      "hashtag\n",
      "#Leadership \n",
      "hashtag\n",
      "#FutureOfTravel \n",
      "hashtag\n",
      "#DigitalInnovation\n",
      "Metadata: {'profile_name': 'cassie-kozyrkov-9531919', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_15\n",
      "Post Text: Just finished writing the next installment for my newsletter, addressing the labor market impact of AI, vanishing rungs on career ladders, why we should worry, and what we can do about it.\n",
      "\n",
      "https://lnkd.in/eNjwgiju\n",
      "\n",
      "If you're not already signed up to decision.substack.com, go ahead and click on that link so you get it in your inbox tomorrow morning. 💌 \n",
      "\n",
      "If you're already a subscriber and you like my musings, a great way to make my day is by pointing one of your friends my way. \n",
      "\n",
      "(P.S. GIF made with Veo 3 - try it out if you haven't yet. Super easy.)\n",
      "Metadata: {'category': 'AI', 'profile_name': 'cassie-kozyrkov-9531919'}\n",
      "----------------------------------------\n",
      "ID: post_16\n",
      "Post Text: Nobody expects the chatbot imposition!\n",
      "https://lnkd.in/eYCsykXQ\n",
      "Metadata: {'category': 'AI', 'profile_name': 'cassie-kozyrkov-9531919'}\n",
      "----------------------------------------\n",
      "ID: post_17\n",
      "Post Text: Will \n",
      "hashtag\n",
      "#AI be a bloodbath for white-collar jobs? \n",
      "hashtag\n",
      "#Anthropic CEO Dario Amodei seems to think so—he made headlines warning that AI could wipe out up to 50% of all entry-level white-collar roles within the next 5 years. While we can debate the exact figure, I won’t quibble: a lot of work is about to be automated.\n",
      "\n",
      "AI isn’t just a helper anymore—it’s becoming a full-blown replacement for the repetitive, digitized, “thunking” tasks that fill so many junior roles. If you’re not paying attention, you’re at risk of missing the train entirely.\n",
      "\n",
      "Here’s the uncomfortable truth: What you see from AI today isn’t the ceiling—it’s the floor. The cutting-edge research is far ahead of what’s in your hands. Even the most notoriously janky AI products—like \n",
      "hashtag\n",
      "#OpenAI’s Operator or \n",
      "hashtag\n",
      "#Google’s Project Mariner—could get a massive capability boost almost overnight, just by cranking up the compute (and, with it, the costs). The real bottleneck? Companies and customers aren’t ready to pay for what’s already possible. We’re stuck in an awkward moment where the tech is ready, but the market—and the culture—aren’t.\n",
      "\n",
      "That gap won’t last forever. AI isn’t some far-off fantasy—it’s the next wave of automation, and it’s already reshaping industries. The problem isn’t that AI is “coming for your job”—it’s that the tasks we once thought were too complex to automate are suddenly on the table. Copying, pasting, filling out forms, writing first drafts of emails—those are the tasks AI is best at. And that means the entry-level training grounds we’ve relied on for generations—where people cut their teeth and build their skills—are vanishing fast. Where will the next generation of talent come from if we don’t rethink our pipelines?\n",
      "\n",
      "Let’s be clear: the next few years will be rough, especially for junior employees. AI is far less of a threat to those with industry experience, deep domain expertise, or strong networks. But if you’re doing work that “anyone can do,” AI will soon be able to do it too. I won’t sugarcoat this, so let me say it again for the folks in the back:\n",
      "\n",
      "⚠️ If anyone can do it, AI will soon be able to do it too. ⚠️\n",
      "\n",
      "If you’re a student or just entering the workforce, now is the time to build relationships, seek out mentors, and cultivate a love of learning—because the treadmill is real, and it’s only speeding up. The future belongs to those who can adapt quickly and learn the new rules of new games.\n",
      "\n",
      "If you’re a leader, this is your moment to lead with compassion. Not everyone loves a constant challenge, and some implicit promises—about stable career paths, about learning your trade and coasting—are about to be broken. AI can empower us to aim higher, but only if we stay nimble. Your job is to build safe learning spaces, empower your teams to experiment with AI tools, and create clear pathways for growth beyond the tasks AI will automate.\n",
      "\n",
      "Let’s not just brace for impact—let’s get ready to lead through it.\n",
      "\n",
      "Subscribe and read on: decision.substack.com\n",
      "Metadata: {'category': 'AI', 'profile_name': 'cassie-kozyrkov-9531919'}\n",
      "----------------------------------------\n",
      "ID: post_18\n",
      "Post Text: The most important difference between a chatbot and an AI agent is that a chatbot can only harm you if you let it. \n",
      "\n",
      "That’s because all a chatbot does is gives you information — unlike an agent, it doesn’t take any actions except potentially filling your head with garbage — so the simplest way to stay safe is to update your digital trust habits.\n",
      "\n",
      "The trick is managing user expectations (hence all those disclaimers that the big providers festoon their chatbots with) but even more importantly managing leader expectations so that they can prepare their teams smartly.\n",
      "\n",
      "What about agentic safety? (Much more complicated) I address that in my Agentic AI for Leaders course -- enroll here for the June cohort: https://lnkd.in/g5X_F4K2\n",
      "\n",
      "If you found this useful, a repost ♻️ makes my heart happy.\n",
      "Metadata: {'category': 'AI', 'profile_name': 'cassie-kozyrkov-9531919'}\n",
      "----------------------------------------\n",
      "ID: post_19\n",
      "Post Text: How will you prevent embarrassment in AI?\n",
      "\n",
      "The answer is… partially. \n",
      "\n",
      "AI will quickly teach you never to repeat the words “I’ve thought of everything.” There's only so much you can do by preparing in advance.\n",
      "\n",
      "When the unexpected rears its ugly head, the best you can hope for is infrastructure (technological and human systems) that reduces the burden of reacting effectively. \n",
      "\n",
      "Any \n",
      "hashtag\n",
      "#leader who doesn't realize this is asking for trouble by implementing \n",
      "hashtag\n",
      "#AI.\n",
      "\n",
      "Read on in my newsletter to find out how \n",
      "hashtag\n",
      "#leaders should think differently about securing a highly performant AI system: https://lnkd.in/eYCsykXQ \n",
      "\n",
      "If you found this useful, a repost ♻️ makes my heart happy.\n",
      "\n",
      "Subscribe for more at decision.substack.com 💌\n",
      "Metadata: {'category': 'AI', 'profile_name': 'cassie-kozyrkov-9531919'}\n",
      "----------------------------------------\n",
      "ID: post_20\n",
      "Post Text: When it comes to AI, AI capability is not AI safety. \n",
      "\n",
      "Many executives conflate the two—assuming that if a system is “smart,” it must also be “safe.” They underestimate how brittle AI systems can be under real-world conditions and overestimate the power of pre-launch testing to catch every possible issue.\n",
      "\n",
      "The reality is, AI risks emerge after deployment, in ways that no amount of foresight or testing can fully anticipate. This is why having a policy layer—a separate control system that governs AI behavior in real-time—is so critical.\n",
      "\n",
      "Read on in my newsletter to understand what I mean and how leaders should think differently about securing a highly performant AI system: https://lnkd.in/eYCsykXQ \n",
      "\n",
      "If you found this useful, a repost ♻️ makes my heart happy.\n",
      "\n",
      "Subscribe for more at decision.substack.com 💌\n",
      "Metadata: {'profile_name': 'cassie-kozyrkov-9531919', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_21\n",
      "Post Text: Here are my top 3 infinitely popcornable moments of chatbots gone wild from the past couple of years:\n",
      "\n",
      "😂 🥉 3rd Place: Delivery company DPD’s chatbot swears, writes poetry, and calls the company “the worst delivery firm in the world.” (2024.)\n",
      "\n",
      "😂 🥈 2rd Place: Air Canada’s chatbot hallucinates a “bereavement fare” policy and is ordered by the British Columbia Civil Resolution Tribunal to uphold it. (2024.)\n",
      "\n",
      "And this next one’s the absolute chef’s kiss of schadenfreude:\n",
      "\n",
      "🤣 🥇 1st Place: Virgin Money’s chatbot reprimands customer for using the word “virgin.” (2025.)\n",
      "\n",
      "Nevertheless, Air Canada, DPD, and Virgin Money are absolute heroes for taking a relatively early plunge into public-facing chatbots. They stubbed their toes in public so the rest of us could learn: highly performant chatbots will never be ready for primetime.\n",
      "\n",
      "But, if we play our cards right, primetime might soon be ready for chatbots… \n",
      "\n",
      "Read on in my newsletter to understand what I mean and how leaders should think differently about securing a highly performant AI system: https://lnkd.in/eYCsykXQ \n",
      "\n",
      "If you found this amusing or useful, a repost makes my heart happy. Let's share the amusefulment!\n",
      "\n",
      "Subscribe for more at decision.substack.com 💌\n",
      "Metadata: {'category': 'AI', 'profile_name': 'cassie-kozyrkov-9531919'}\n",
      "----------------------------------------\n",
      "ID: post_22\n",
      "Post Text: Can one person build a billion-dollar company with \n",
      "hashtag\n",
      "#AI \n",
      "hashtag\n",
      "#agents? Maybe.\n",
      "\n",
      "Can they build it in high-risk, high-compliance fields like medicine, law, or finance? I have my doubts.\n",
      "\n",
      "hashtag\n",
      "#Solopreneurs, armed with AI copilots, can now do the work of entire teams in *some* industries. If you're building content or productivity tools, that may be within reach. \n",
      "\n",
      "But in high-risk sectors, the bottleneck isn’t talent or tooling. It’s everything else — security, compliance, auditability, and the kind of safeguards that let you operate responsibly under scrutiny.\n",
      "\n",
      "Getting something to work is one thing. Getting it to scale safely enough to be adopted by enterprise buyers is another.\n",
      "\n",
      "Want to know more? Check out this piece in ZDNET (I'm quoted!) where I got to speak with Joe McKendrick about the opportunities lie for AI-powered solo founders:\n",
      "\n",
      "https://lnkd.in/e9R_trh4\n",
      "Metadata: {'category': 'AI', 'profile_name': 'cassie-kozyrkov-9531919'}\n",
      "----------------------------------------\n",
      "ID: post_23\n",
      "Post Text: There’s a new breed of GenAI Application Engineers who can build more-powerful applications faster than was possible before, thanks to generative AI. Individuals who can play this role are highly sought-after by businesses, but the job description is still coming into focus. Let me describe their key skills, as well as the sorts of interview questions I use to identify them.\n",
      "\n",
      "Skilled GenAI Application Engineers meet two primary criteria: (i) They are able to use the new AI building blocks to quickly build powerful applications. (ii) They are able to use AI assistance to carry out rapid engineering, building software systems in dramatically less time than was possible before. In addition, good product/design instincts are a significant bonus.\n",
      "\n",
      "AI building blocks. If you own a lot of copies of only a single type of Lego brick, you might be able to build some basic structures. But if you own many types of bricks, you can combine them rapidly to form complex, functional structures. Software frameworks, SDKs, and other such tools are like that. If all you know is how to call a large language model (LLM) API, that's a great start. But if you have a broad range of building block types — such as prompting techniques, agentic frameworks, evals, guardrails, RAG, voice stack, async programming, data extraction, embeddings/vectorDBs, model fine tuning, graphDB usage with LLMs, agentic browser/computer use, MCP, reasoning models, and so on — then you can create much richer combinations of building blocks.\n",
      "\n",
      "The number of powerful AI building blocks continues to grow rapidly. But as open-source contributors and businesses make more building blocks available, staying on top of what is available helps you keep on expanding what you can build. Even though new building blocks are created, many building blocks from 1 to 2 years ago (such as eval techniques or frameworks for using vectorDBs) are still very relevant today.\n",
      "\n",
      "AI-assisted coding. AI-assisted coding tools enable developers to be far more productive, and such tools are advancing rapidly. Github Copilot, first announced in 2021 (and made widely available in 2022), pioneered modern code autocompletion. But shortly after, a new breed of AI-enabled IDEs such as Cursor and Windsurf offered much better code-QA and code generation. As LLMs improved, these AI-assisted coding tools that were built on them improved as well.\n",
      "\n",
      "Now we have highly agentic coding assistants such as OpenAI’s Codex and Anthropic’s Claude Code (which I really enjoy using and find impressive in its ability to write code, test, and debug autonomously for many iterations). In the hands of skilled engineers — who don’t just “vibe code” but deeply understand AI and software architecture fundamentals and can steer a system toward a thoughtfully selected product goal — these tools make it possible to build software with unmatched speed and efficiency.\n",
      "\n",
      "[Truncated due to length limit. Full post: https://lnkd.in/gsztgv2f ]\n",
      "Metadata: {'profile_name': 'andrewyng', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_24\n",
      "Post Text: Learn to build and deploy GenAI pipelines in \"Orchestrating Workflows for GenAI Applications\", built in partnership with Astronomer and taught by Kenten Danas, the company's DevRel Senior Manager, and Tamara Janina Fingerlin, developer advocate. \n",
      "\n",
      "Many GenAI applications require executing a pipeline comprising many steps. For example, a RAG app for recommending books might ingest and embed book descriptions, store the embeddings in a vector database, and later use the database to retrieve and recommend specific books based on a user query. After having prototyped this -- maybe in a Jupyter notebook -- how do you turn this into a reliable, repeatable workflow to run in production? \n",
      "\n",
      "In this short course, you’ll learn to build reliable GenAI pipelines and orchestrate them using the popular open-source tool Airflow 3.0. You’ll learn to break down a workflow into discrete tasks so that an orchestration framework can schedule tasks to run in the right order at the right time (using time-based or data-aware triggers) and execute tasks in parallel when possible. It can also use retries to recover gracefully from failure (such as transient API rate limits) and provide observability (using Airflow UI) to help you track the status of the pipeline. You'll do this by using Airflow dags, which helps sequence tasks that need to run in a specific order, with clear task dependencies. \n",
      "\n",
      "By the end of this course, you’ll know how to turn your prototype Jupyter notebook or Python script into production-ready workflow. \n",
      "\n",
      "Please sign up here: https://lnkd.in/gK5HAsfZ\n",
      "Metadata: {'category': 'AI', 'profile_name': 'andrewyng'}\n",
      "----------------------------------------\n",
      "ID: post_25\n",
      "Post Text: Hanging out with Justin Uberti, OpenAI’s head of realtime AI, responsible for the company’s voice AI products. One thing both of us agree on: while some things in AI are overhyped, voice applications seem underhyped right now. The application opportunities seem larger than the amount of developer or business attention on this right now.\n",
      "Metadata: {'category': 'AI', 'profile_name': 'andrewyng'}\n",
      "----------------------------------------\n",
      "ID: post_26\n",
      "Post Text: New short course: DSPy: Build and Optimize Agentic Apps\n",
      "\n",
      "DSPy is a powerful open-source framework for automatically tuning prompts for GenAI applications. In this course, you'll learn to use DSPy, together with MLflow. This is built in partnership with Databricks and taught by Chen Qian, co-lead of the DSPy framework.\n",
      "\n",
      "Many AI builders spend hours hand-tuning prompts. When given a set of evals, DSPy automates this process. It’s especially useful for optimizing prompts, including few-shot prompts, in complex agentic AI workflows. Further, if you switch an application to a newer LLM, performance can degrade if your prompts were optimized to the previous model. DSPy automatically optimizes the entire system for the new LLM as well, using just a few evaluation examples.\n",
      "\n",
      "This course teaches DSPy works, and best practices for using it. You’ll write programs using DSPy’s signature-based programming model, debug them with MLflow tracing -- to gain visibility into how different parts of a pipeline, as well as how the overall system, are performing -- and automatically improve their accuracy with DSPy Optimizer.\n",
      "\n",
      "Please sign up here: https://lnkd.in/gdjae8AX\n",
      "Metadata: {'profile_name': 'andrewyng', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_27\n",
      "Post Text: Everyone should learn to code with AI! At AI Fund, everyone - not just engineers - can vibe code or use AI assistance to code. This has been great for our creativity and productivity. I hope more teams will empower everyone to build with AI. Please watch the video for details.\n",
      "Metadata: {'profile_name': 'andrewyng', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_28\n",
      "Post Text: I am alarmed by the proposed cuts to U.S. funding for basic research, and the impact this would have for U.S. competitiveness in AI and other areas. Funding research that is openly shared benefits the whole world, but the nation it benefits most is the one where the research is done.\n",
      "\n",
      "If not for funding for my early work in deep learning from the National Science Foundation (NSF) and Defense Advanced Research Projects Agency (DARPA), which disburse a good deal of U.S. research funding, I would not have discovered lessons about scaling that led me to pitch starting Google Brain to scale up deep learning. I am worried that cuts to funding for basic science will lead the U.S. — and also the world — to miss out on the next set of ideas.\n",
      "\n",
      "In fact, such funding benefits the U.S. more than any other nation. Scientific research brings the greatest benefit to the country where the work happens because (i) the new knowledge diffuses fastest within that country, and (ii) the process of doing research creates new talent for that nation.\n",
      "\n",
      "Why does most innovation in generative AI still happen in Silicon Valley? Because two teams based in this area — Google Brain, which invented the transformer network, and OpenAI, which scaled it up — did a lot of the early work. Subsequently, team members moved to other nearby businesses, started competitors, or worked with local universities. Further, local social networks rapidly diffused the knowledge through casual coffee meetings, local conferences, and even children’s play dates, where parents of like-aged kids meet and discuss technical ideas. In this way, the knowledge spread faster within Silicon Valley than to other geographies.\n",
      "\n",
      "In a similar vein, research done in the U.S. diffuses to others in the U.S. much faster than to other geographic areas. This is particularly true when the research is openly shared through papers and/or open source: If researchers have permission to talk about an idea, they can share much more information, such as tips and tricks for how to really make an algorithm work, more quickly. It also lets others figure out faster who can answer their questions. Diffusion of knowledge created in academic environments is especially fast. Academia tends to be completely open, and students and professors, unlike employees of many companies, have full permission to talk about their work.\n",
      "\n",
      "Thus funding basic research in the U.S. benefits the U.S. most, and also benefits our allies. It is true that openness benefits our adversaries, too. But as a subcommittee of the U.S. House of Representatives committee on science, space, and technology points out, “... open sharing of fundamental research is [not] without risk. Rather, ... openness in research is so important to competitiveness and security that it warrants the risk that adversaries may benefit from scientific openness as well.”\n",
      "\n",
      "[Truncated due to length limit. Full post: https://lnkd.in/gQfYfxAF ]\n",
      "Metadata: {'category': 'AI', 'profile_name': 'andrewyng'}\n",
      "----------------------------------------\n",
      "ID: post_29\n",
      "Post Text: Agentic Document Extraction just got much faster! From previous 135sec median processing time down to 8sec. Extracts not just text but diagrams, charts, and form fields from PDFs to give LLM-ready output. Please see the video for details and some application ideas.\n",
      "Metadata: {'category': 'AI', 'profile_name': 'andrewyng'}\n",
      "----------------------------------------\n",
      "ID: post_30\n",
      "Post Text: In the age of AI, large corporations — not just startups — can move fast. I often speak with large companies’ C-suite and Boards about AI strategy and implementation, and would like to share some ideas that are applicable to big companies. One key is to create an environment where small, scrappy teams don’t need permission to innovate. Let me explain.\n",
      "\n",
      "Large companies are slower than startups for many reasons. But why are even 3-person, scrappy teams within large companies slower than startups of a similar size? One major reason is that large companies have more to lose, and cannot afford for a small team to build and ship a feature that leaks sensitive information, damages the company brand, hurts revenue, invites regulatory scrutiny, or otherwise damages an important part of the business. To prevent these outcomes, I have seen companies require privacy review, marketing review, financial review, legal review, and so on before a team can ship anything. But if engineers need sign-off from 5 vice presidents before they’re even allowed to launch an MVP (minimum viable product) to run an experiment, how can they ever discover what customers want, iterate quickly, or invent any meaningful new product?\n",
      "\n",
      "Thanks to AI-assisted coding, the world now has a capability to build software prototypes really fast. But many large companies’ processes – designed to protect against legitimate downside risks – make them unable to take advantage of this capability. In contrast, in small startups with no revenue, no customers, and no brand reputation the downside is limited. In fact, going out of business is a very real possibility anyway, so moving fast makes a superior tradeoff to moving slowly to protect against downside risk. In the worst case, it might invent a new way to go out of business, but in a good case, it might become very valuable.\n",
      "\n",
      "Fortunately, large companies have a way out of this conundrum. They can create a sandbox environment for teams to experiment in a way that strictly limits the downside risk. Then those teams can go much faster and not have to slow down to get anyone’s permission.\n",
      "\n",
      "The sandbox environment can be a set of written policies, not necessarily a software implementation of a sandbox. For example, it may permit a team to test the nascent product only on employees of the company and perhaps alpha testers who have signed an NDA, and give no access to sensitive information. It may be allowed to launch product experiments only under newly created brands not tied directly to the company. Perhaps it must operate within a pre-allocated budget for compute.\n",
      "\n",
      "Within this sandbox, there can be broad scope for experimentation. Further, when a prototype shows sufficient promise to bring it to scale, the company can then invest in making sure the software is reliable, secure, treats sensitive information appropriately, is consistent with the company brand, and so on.\n",
      "\n",
      "[At length limit. Full text: https://lnkd.in/gxhtQa-A ]\n",
      "Metadata: {'category': 'AI', 'profile_name': 'andrewyng'}\n",
      "----------------------------------------\n",
      "ID: post_31\n",
      "Post Text: New Course: Reinforcement Fine-Tuning LLMs with GRPO! \n",
      "\n",
      "Learn to use reinforcement learning to improve your LLM performance in this short course, built in collaboration with Predibase, and taught by Travis Addair, its Co-Founder and CTO, and Arnav Garg, its Senior Engineer and Machine Learning Lead.\n",
      "\n",
      "Reasoning models have been one of the most important developments in LLMs. Reinforcement Fine-Tuning (RFT) uses rewards to encourage LLMs to find solutions to multi-step reasoning tasks such as solving math problems and debugging code - without needing pre-existing training examples like in traditional supervised fine-tuning.\n",
      "\n",
      "Group Relative Policy Optimization (GRPO) is a reinforcement fine-tuning algorithm gaining rapid adoption. Developed by the DeepSeek team and used to train the R1 reasoning model, GRPO uses reward functions that you can write in Python to assign rewards to model responses. It’s beneficial for tasks with verifiable outcomes and can work well even with fewer than 100 training examples. It can also significantly improve the reasoning ability of smaller LLMs, making applications faster and more cost effective.\n",
      "\n",
      "In this course, you’ll take a technical deep dive into RFT with GRPO. You’ll learn to build reward functions that you can use in the GRPO training process to guide an LLM toward better performance on multi-step reasoning tasks.\n",
      "\n",
      "In detail, you’ll:\n",
      "- Learn when reinforcement fine-tuning is a better fit than supervised fine-tuning, especially for tasks involving multi-step reasoning or limited labeled data.\n",
      "- Understand how GRPO uses programmable reward functions as a more scalable alternative to the human feedback required for other reinforcement learning algorithms, such as RLHF and DPO.\n",
      "- Frame the Wordle game as a reinforcement fine-tuning problem and see how an LLM can learn to plan, analyze feedback, and improve its strategy over time.\n",
      "- Design reward functions that power the reinforcement fine-tuning process.\n",
      "- Learn techniques for evaluating more subjective tasks, such as rating the quality of a text summary, using an LLM as a judge.\n",
      "- Understand why reward hacking happens and how to avoid it by adding penalty functions to discourage undesirable behaviors.\n",
      "- Learn the four key components of the loss calculation in the GRPO algorithm: token probability distribution ratios, advantages, clipping, and KL-divergence.\n",
      "- Launch reinforcement fine-tuning jobs using Predibase’s hosted training services.\n",
      "\n",
      "By the end of this course, you’ll be able to build and fine-tune LLMs using reinforcement learning to improve reasoning without relying on large labeled datasets or subjective human feedback.\n",
      "\n",
      "Please sign up here: https://lnkd.in/gpHQUier\n",
      "Metadata: {'profile_name': 'andrewyng', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_32\n",
      "Post Text: AI’s ability to make tasks not just cheaper, but also faster, is underrated in its importance in creating business value.\n",
      "\n",
      "For the task of writing code, AI is a game-changer. It takes so much less effort — and is so much cheaper — to write software with AI assistance than without. But beyond reducing the cost of writing software, AI is shortening the time from idea to working prototype, and the ability to test ideas faster is changing how teams explore and invent. When you can test 20 ideas per month, it dramatically changes what you can do compared to testing 1 idea per month. This is a benefit that comes from AI-enabled speed rather than AI-enabled cost reduction.\n",
      "\n",
      "That AI-enabled automation can reduce costs is well understood. For example, providing automated customer service is cheaper than operating human-staffed call centers. Many businesses are more willing to invest in growth than just in cost savings; and, when a task becomes cheaper, some businesses will do a lot more of it, thus creating growth. But another recipe for growth is underrated: Making certain tasks much faster (whether or not they also become cheaper) can create significant new value.\n",
      "\n",
      "I see this pattern across more and more businesses. Consider the following scenarios:\n",
      "- If a lender can approve loans in minutes using AI, rather than days waiting for a human to review them, this creates more borrowing opportunities (and also lets the lender deploy its capital faster). Even if human-in-the-loop review is needed, using AI to get the most important information to the reviewer might speed things up. \n",
      "- If an academic institution gives homework feedback to students in minutes (via autograding) rather than days (via human grading), the rapid feedback facilitates better learning.\n",
      "- If an online seller can approve purchases faster, this can lead to more sales. For example, many platforms that accept online ad purchases have an approval process that can take hours or days; if approvals can be done faster, they can earn revenue faster. This also enables customers to test ideas faster.\n",
      "- If a company’s sales department can prioritize leads and respond to prospective customers in minutes or hours rather than days — closer to when the customers’ buying intent first led them to contact the company — sales representatives might close more deals. Likewise, a business that can respond more quickly to requests for proposals may win more deals.\n",
      "\n",
      "I’ve written previously about looking at the tasks a company does to explore where AI can help. Many teams already do this with an eye toward making tasks cheaper, either to save costs or to do those tasks many more times. If you’re doing this exercise, consider also whether AI can significantly speed up certain tasks. One place to examine is the sequence of tasks on the path to earning revenue. If some of the steps can be sped up, perhaps this can help revenue growth.\n",
      "\n",
      "[Edited for length; full text: https://lnkd.in/gBCc2FTn ]\n",
      "Metadata: {'profile_name': 'andrewyng', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_33\n",
      "Post Text: New course: MCP: Build Rich-Context AI Apps with Anthropic. Learn to build AI apps that access tools, data, and prompts using the Model Context Protocol in this short course, created in partnership with Anthropic and taught by Elie Schoppik, its Head of Technical Education.\n",
      "\n",
      "Connecting AI applications to external systems that bring rich context to LLM-based applications has often meant writing custom integrations for each use case. MCP is an open protocol that standardizes how LLMs access tools, data, and prompts from external sources, and simplifies how you provide context to your LLM-based applications. For example, you can provide context via third-party tools that let your LLM make API calls to search the web, access data from local docs, retrieve code from a GitHub repo, and so on.\n",
      "\n",
      "MCP, developed by Anthropic, is based on a client-server architecture that defines the communication details between an MCP client, hosted inside the AI application, and an MCP server that exposes tools, resources, and prompt templates. The server can be a subprocess launched by the client that runs locally or an independent process running remotely.\n",
      "\n",
      "In this hands-on course, you'll learn the core architecture behind MCP. You’ll create an MCP-compatible chatbot, build and deploy an MCP server, and connect the chatbot to your MCP server and other open-source servers.\n",
      "\n",
      "Here’s what you’ll do:\n",
      "- Understand why MCP makes AI development less fragmented and standardizes connections between AI applications and external data sources\n",
      "- Learn the core components of the client-server architecture of MCP and the underlying communication mechanism\n",
      "- Build a chatbot with custom tools for searching academic papers, and transform it into an MCP-compatible application\n",
      "- Build a local MCP server that exposes tools, resources, and prompt templates using FastMCP, and test it using MCP Inspector\n",
      "- Create an MCP client inside your chatbot to dynamically connect to your server\n",
      "- Connect your chatbot to reference servers built by Anthropic’s MCP team, such as filesystem, which implements filesystem operations, and fetch, which extracts contents from the web as markdown\n",
      "- Configure Claude Desktop to connect to your server and others, and explore how it abstracts away the low-level logic of MCP clients\n",
      "- Deploy your MCP server remotely and test it with the Inspector or other MCP-compatible applications\n",
      "- Learn about the roadmap for future MCP development, such as multi-agent architecture, MCP registry API, server discovery, authorization, and authentication\n",
      "\n",
      "MCP is an exciting and important technology that lets you build rich-context AI applications that connect to a growing ecosystem of MCP servers, with minimal integration work.\n",
      "\n",
      "Please sign up here! https://lnkd.in/gRaYPn_X\n",
      "Metadata: {'category': 'AI', 'profile_name': 'andrewyng'}\n",
      "----------------------------------------\n",
      "ID: post_34\n",
      "Post Text: I’m delighted to announce that AI Fund has closed $190M for our new fund, in an oversubscribed round. I look forward to working with many more builders to create new companies that serve humanity.\n",
      "\n",
      "AI Fund isn’t a traditional venture capital firm that invests in existing businesses. Instead, we are a venture builder (also called a venture studio): We co-found AI companies, so our team is directly involved in writing code, talking to customers to get feedback, iterating on product designs, preparing market analyses, and so on. We have a lot of fun building multiple AI products at a time, and thus live daily the emerging AI startup best practices.\n",
      "\n",
      "Many factors go into the success of a startup. But if I had to pick just one, it would be speed. Startups live or die based on their ability to make good decisions and execute fast, which has been a recurring theme of my posts here as well.\n",
      "\n",
      "If you are building an AI startup, here are some ideas to consider:\n",
      "- A startup with a small team that pursues one focused, concrete idea can move really fast. Rather than hedging, it is often better to pursue one hypothesis (for example, build one concrete product) but also be willing to switch quickly to a different hypothesis (say, change what features you decide to build) if the data that comes back indicates the original hypothesis is flawed. Concreteness gets you speed!\n",
      "- A subject matter expert’s gut is remarkably good at making quick decisions. Obviously, there’s a role for data and user studies as well. But if you’re deciding whether to build feature A or B, or to sell first to user persona X or Y, sometimes a domain expert’s gut will point to a quick decision that you can execute and validate or falsify. Trusting a domain expert’s gut gets you speed!\n",
      "- AI-assisted coding is making prototyping faster than ever before. Yes, AI assistance is speeding up building reliable, enterprise-grade applications and maintaining legacy codebases. But the acceleration it brings to building stand-alone prototypes is far greater. This is because stand-alone prototypes have low requirements for reliability, integration, or even security (if, say, you run them in a sandbox environment). This lets us prototype and test at a ferocious velocity. AI-assisted coding (including vibe coding, where you might barely look at the code) gets you speed!\n",
      "- Finally, with faster prototyping, the bottleneck shifts to getting feedback from users. A single learning cycle might consist of (i) building a prototype and (ii) getting user feedback to inform the next iteration. Since (i) is now much faster, accelerating (ii) is growing in importance. This means teams that are skilled at finding prospective customers and getting their feedback in hours/days rather than weeks can go faster. \n",
      "\n",
      "I’m grateful to AI Fund’s investors, team, and entrepreneur partners for working with us. There is much ahead to build!\n",
      "\n",
      "[Shortened due to length limit. Full text: https://lnkd.in/gBaqAAcF ]\n",
      "Metadata: {'profile_name': 'andrewyng', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_35\n",
      "Post Text: Learn to build conversational AI voice agents in \"Building AI Voice Agents for Production\", created in collaboration with LiveKit and RealAvatar, and taught by Russ d'Sa (Co-founder & CEO of LiveKit), Shayne Parmelee (Developer Advocate, LiveKit), and Nedelina Teneva, PhD (Head of AI at RealAvatar, an AI Fund portfolio company).\n",
      "\n",
      "Voice agents combine speech and reasoning capabilities to enable real-time conversations. They're already being used to support customer service, to improve accessibility in healthcare, for entertainment applications, and for talk therapy.\n",
      "\n",
      "In this course, you’ll learn to build voice agents that listen, reason, and respond naturally. You’ll follow the architecture used to create the \"AI Andrew\" Avatar, a collaborative project between DeepLearning.AI and RealAvatar that responds to users in what sounds like my voice. You’ll build a voice agent from scratch and deploy it to the cloud, enabling support for many simultaneous users.\n",
      "\n",
      "What you’ll learn:\n",
      "- Understand the fundamentals of voice agents, including key components like speech-to-text (STT), text-to-speech (TTS), and LLMs, and how latency is introduced at each layer.\n",
      "- Explore voice agent architectures and the trade-offs between modular pipelines and speech-to-speech APIs.\n",
      "- Explore how platforms like LiveKit mitigate latency issues with optimized networking infrastructure and low-latency communication protocols.\n",
      "- Learn how to connect client devices to voice agents using WebRTC—and why it outperforms HTTP and WebSocket for low-latency audio streaming.\n",
      "- Incorporate voice activity detection (VAD), end-of-turn detection, and context management to detect turns, handle interruptions, and manage conversational flow.\n",
      "- Understand the trade-offs between latency, quality, and cost in an example in which you build a voice agent and change its voice.\n",
      "- Equip your agent with metrics to measure latency at each stage of the voice pipeline and learn the key levers you can pull to make your agent faster and more responsive.\n",
      "\n",
      "The voice agents built in this course also incorporate voice technology from ElevenLabs, a supporting contributor to the project.\n",
      "\n",
      "By the end of this course, you'll have learned the components of an AI voice agent pipeline, combined them into a system with low-latency communication, and deployed them on cloud infrastructure so it scales to many users.\n",
      "\n",
      "I’m looking forward to seeing what voice agents you build from this course!\n",
      "\n",
      "Please sign up here: https://lnkd.in/gEBXAGH8\n",
      "Metadata: {'profile_name': 'andrewyng', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_36\n",
      "Post Text: In addition to being a great investor, Warren Buffett has also been a great teacher, and I'm grateful to have learned a lot from him. For example, one concept I refer to frequently at AI Fund is the Circle of Competence, meaning we figure out what we're good at and what we're not, and act accordingly. His stepping down from Berkshire Hathaway will be the end of an era!\n",
      "Metadata: {'category': 'AI', 'profile_name': 'andrewyng'}\n",
      "----------------------------------------\n",
      "ID: post_37\n",
      "Post Text: I hope we can empower everyone to build with AI. Starting from K-12, we should teach every student AI enabled coding, since this will enable them to become more productive and more empowered adults. But there is a huge shortage of computer science (CS) teachers. I recently spoke with high school basketball coach Kyle Creasy, who graduated with a B.A. in Physical Education in 2023. Until two years ago, he had never written a line of Python. Now — with help from AI — he not only writes code, he also teaches CS. I found Kyle’s story inspiring as a model for scaling up CS education in the primary- and secondary-school levels.\n",
      "\n",
      "Kyle’s success has been with the support of Kira Learning (an AI Fund portfolio company), whose founders Andrea Pasinetti and Jagriti Agrawal have created a compelling vision for CS education. In K-12 classrooms, teachers play a huge social-emotional support role, for example, encouraging students and helping them when they stumble. In addition, they are expected to be subject-matter experts who can deliver the content needed for their subject. Kira Learning uses digital content delivery — educational videos, autograded quizzes, and AI-enabled chatbots to answer students' questions but without giving away homework answers — so the teacher can focus on social-emotional support. While these are still early days, it appears to be working!\n",
      "\n",
      "A key to making this possible is the hyperpersonalization that is now possible with AI (in contrast to the older idea of the flipped classroom, which had limited adoption). For example, when assigned a problem in an online coding environment, if a student writes this buggy line of Python code\n",
      "\n",
      "best_$alty_snack = 'potato chips'\n",
      "\n",
      "Kira Learning’s AI system can spot the problem and directly tell the teacher that $ is an invalid character in a variable name. It can also suggest a specific question for the teacher to ask the student to help get them unstuck, like “Can you identify what characters are allowed in variable names?” Whereas AI can directly deliver personalized advice to students, the fact that it is now helping teachers also deliver personalized support will really help in K-12.\n",
      "\n",
      "Additionally, agentic workflows can automate a lot of teachers’ repetitive tasks. For example, when designing a curriculum, it’s time-consuming to align the content to educational standards (such as the Common Core in the United States, or the AP CS standard for many CS classes). Having an AI system carry out tasks like these is already proving helpful for teachers.\n",
      "\n",
      "Since learning to code, Kyle has built many pieces of software. He proudly showed me an analysis he generated in matplotlib of his basketball players’ attempts to shoot three-pointers (shown above), which in turn is affecting the team’s strategy on the court. One lesson is clear: When a basketball coach learns to code, they become a better basketball coach! \n",
      "\n",
      "[Reached length limit. Full text: https://lnkd.in/gthKuC5Q ]\n",
      "Metadata: {'category': 'AI', 'profile_name': 'andrewyng'}\n",
      "----------------------------------------\n",
      "ID: post_38\n",
      "Post Text: Even though I’m a much better Python than JavaScript developer, with AI assistance, I’ve been writing a lot of JavaScript code recently. AI-assisted coding, including vibe coding, is making specific programming languages less important, even though learning one is still helpful to make sure you understand the key concepts. This is helping many developers write code in languages we’re not familiar with, which lets us get code working in many more contexts!\n",
      "\n",
      "My background is in machine learning engineering and back-end development, but AI-assisted coding is making it easy for me to build front-end systems (the part of a website or app that users interact with) using JavaScript (JS) or TypeScript (TS), languages that I am weak in. Generative AI is making syntax less important, so we can all simultaneously be Python, JS, TS, C++, Java, and even Cobol developers. Perhaps one day, instead of being “Python developers\" or “C++ developers,” many more of us will just be “developers”!\n",
      "\n",
      "But understanding the concepts behind different languages is still important. That’s why learning at least one language like Python still offers a great foundation for prompting LLMs to generate code in Python and other languages. If you move from one programming language to another that carries out similar tasks but with different syntax — say, from JS to TS, or C++ to Java, or Rust to Go — once you’ve learned the first set of concepts, you’ll know a lot of the concepts needed to prompt an LLM to code in the second language. (Although TensorFlow and PyTorch are not programming languages, learning the concepts of deep learning behind TensorFlow will also make it much easier to get an LLM to write PyTorch code for you, and vice versa!) In addition, you’ll be able to understand much of the generated code (perhaps with a little LLM assistance).\n",
      "\n",
      "Different programming languages reflect different views of how to organize computation, and understanding the concepts is still important. For example, someone who does not understand arrays, dictionaries, caches, and memory will be less effective at getting an LLM to write code in most languages.\n",
      "\n",
      "Similarly, a Python developer who moves toward doing more front-end programming with JS would benefit from learning the concepts behind front-end systems. For example, if you want an LLM to build a front end using the React framework, it will benefit you to understand how React breaks front ends into reusable UI components, and how it updates the DOM data structure that determines what web pages look like. This lets you prompt the LLM much more precisely, and helps you understand how to fix issues if something goes wrong. Similarly, if you want an LLM to help you write code in CUDA or ROCm, it helps to understand how GPUs organize compute and memory.\n",
      "\n",
      "[Reached length limit; full text: https://lnkd.in/dS_buaTu ]\n",
      "Metadata: {'profile_name': 'andrewyng', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_39\n",
      "Post Text: New short course: Building Code Agents with Hugging Face smolagents!\n",
      "\n",
      "Learn how to build code agents in this course, created in collaboration with Hugging Face, and taught by Thomas Wolf, its co-founder and CSO, and Aymeric Roucher, Hugging Face’s Project Lead on Agents.\n",
      "\n",
      "Tool-calling agents use LLMs to generate multiple function calls sequentially to complete a complex sequence of tasks. They generate one function call, execute it, observe, reason, and decide what to do next. Code agents take a different approach. They consolidate all these calls into a single block of code, letting the LLM lay out an entire action plan at once, which can be executed efficiently to provide more reliable results.\n",
      "\n",
      "You’ll learn how to code agents using smolagents, a lightweight agentic framework from Hugging Face. Along the way, you’ll learn how to run LLM-generated code safely and develop an evaluation system to optimize your code agent for production.\n",
      "\n",
      "In detail, you’ll learn:\n",
      "- How agentic systems have evolved, gaining greater levels of agency over time—and why code agents are a next step.\n",
      "- How code agents write their actions in code.\n",
      "- When code agents outperform function-calling agents.\n",
      "- How to run code agents safely in your system using a constrained Python interpreter and sandboxing using E2B.\n",
      "- To trace, debug, and assess the code agent to optimize its behaviours for complex requests.\n",
      "- How to build a research multi-agent system that can find information online and organize it into an interactive report.\n",
      "\n",
      "By the end of this course, you’ll know how to build and run code agents using smolagents, and deploy them safely with a structured evaluation system in your projects.\n",
      "\n",
      "Please sign up here! https://lnkd.in/g2Td5WqD\n",
      "Metadata: {'profile_name': 'andrewyng', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_40\n",
      "Post Text: I’ve noticed that many GenAI application projects put in automated evaluations (evals) of the system’s output probably later — and rely on humans to manually examine and judge outputs longer — than they should. This is because building evals is viewed as a massive investment (say, creating 100 or 1,000 examples, and designing and validating metrics) and there’s never a convenient moment to put in that up-front cost. Instead, I encourage teams to think of building evals as an iterative process. It’s okay to start with a quick-and-dirty implementation (say, 5 examples with unoptimized metrics) and then iterate and improve over time. This allows you to gradually shift the burden of evaluations away from humans and toward automated evals.\n",
      "\n",
      "I wrote previously in The Batch about the importance and difficulty of creating evals. Say you’re building a customer-service chatbot that responds to users in free text. There’s no single right answer, so many teams end up having humans pore over dozens of example outputs with every update to judge if it improved the system. While techniques like LLM-as-judge are helpful, the details of getting this to work well (such as what prompt to use, what context to give the judge, and so on) are finicky to get right. All this contributes to the impression that building evals requires a large up-front investment, and thus on any given day, a team can make more progress by relying on human judges than figuring out how to build automated evals.\n",
      "\n",
      "I encourage you to approach building evals differently. It’s okay to build quick evals that are only partial, incomplete, and noisy measures of the system’s performance, and to iteratively improve them. They can be a complement to, rather than replacement for, manual evaluations. Over time, you can gradually tune the evaluation methodology to close the gap between the evals’ output and human judgments. For example:\n",
      "- It’s okay to start with very few examples in the eval set, say 5, and gradually add to them over time — or subtract them if you find that some examples are too easy or too hard, and not useful for distinguishing between the performance of different versions of your system.\n",
      "- It’s okay to start with evals that measure only a subset of the dimensions of performance you care about, or measure narrow cues that you believe are correlated with, but don’t fully capture, system performance. For example if, at a certain moment in the conversation, your customer-support agent is supposed to (i) call an API to issue a refund and (ii) generate an appropriate message to the user, you might start off measuring only whether or not it calls the API correctly and not worry about the message. Or if, at a certain moment, your chatbot should recommend a specific product, a basic eval could measure whether or not the chatbot mentions that product without worrying about what it says about it.\n",
      "\n",
      "[Truncated due to length limit. Full text: https://lnkd.in/gygj3y7w ]\n",
      "Metadata: {'category': 'AI', 'profile_name': 'andrewyng'}\n",
      "----------------------------------------\n",
      "ID: post_41\n",
      "Post Text: New Short Course: Building AI Browser Agents! \n",
      "\n",
      "Learn how to build AI agents that interact with and take actions on websites in this course, created in partnership with AGI, Inc. and taught by Div Garg @DivGarg_ and Naman Garg, Co-founders of AGI Inc.\n",
      "\n",
      "AI browser agents can log into websites, fill out forms, click through web pages, or even place orders online for you. They use both visual information, like screenshots, and structural data, like the HTML or Document Object Model (DOM) of a web page, to reason and take action.\n",
      "\n",
      "With the complexity of webpages and multiple possible actions at each step, it can be challenging for an AI browser agent to complete an assigned task. Because these agents run long action sequences, a single error—like clicking the wrong button or misreading a field—can lead to unexpected outcomes or errors that compound over time.\n",
      "\n",
      "In this course, you'll understand how autonomous web agents work, their current limitations, and how AgentQ enables them to improve through self-correction.\n",
      "\n",
      "In detail, you'll:\n",
      "- Learn what web agents are, how they automate tasks online, their architecture, key components, limitations, and an overview of their decision-making strategies.\n",
      "- Build a web agent that can scrape DeepLearning.AI's website and return course recommendations in a structured output format.\n",
      "- Build an autonomous web agent that can execute multiple tasks, such as finding and summarizing webpages, filling out a form, and signing up for a newsletter.\n",
      "- Explore AgentQ, a framework that enables agents to self-correct by combining Monte Carlo Tree Search (MCTS), a self-critique mechanism for continuous improvement, and Direct Preference Optimization (DPO).\n",
      "- Deep dive into MCTS, learn how it finds an effective path, illustrated by an example of Gridworld animation, and use AgentQ to complete web tasks.\n",
      "- Understand AI agents' current state and future directions—including key factors shaping their evolution, such as hardware, algorithm innovation, and data availability.\n",
      "\n",
      "By the end of this course, you will have hands-on experience building browser agents and a deeper understanding of how to make them more robust and reliable.\n",
      "\n",
      "Please sign up here! https://lnkd.in/g7qMy9ww\n",
      "Metadata: {'profile_name': 'andrewyng', 'category': 'AI'}\n",
      "----------------------------------------\n",
      "ID: post_42\n",
      "Post Text: My thoughts on Tariffs' impact on AI (written for The Batch before the 90 day pause; but many of the points are still relevant depending on what happens next).\n",
      "Metadata: {'profile_name': 'andrewyng', 'category': 'AI'}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Reconnect to the ChromaDB collection\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "collection = client.get_collection(name=\"posts_collection\")\n",
    "\n",
    "# Get a few entries to inspect (e.g., first 5)\n",
    "results = collection.get(\n",
    "    ids=[f\"post_{i}\" for i in range(80)],  # Adjust the range as needed\n",
    "    include=[\"documents\", \"metadatas\", \"embeddings\"]\n",
    ")\n",
    "\n",
    "# Print them nicely\n",
    "for i in range(len(results[\"ids\"])):\n",
    "    print(f\"ID: {results['ids'][i]}\")\n",
    "    print(f\"Post Text: {results['documents'][i]}\")\n",
    "    print(f\"Metadata: {results['metadatas'][i]}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f9ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector (first 5 values): [ 1.70568079e-02  4.12511155e-02  8.53130594e-03  9.25791562e-02\n",
      " -3.10870651e-02 -1.37444437e-01  1.26389891e-01 -6.08916767e-03\n",
      "  2.68113576e-02 -2.79910062e-02 -6.71681389e-02 -8.77571851e-02\n",
      " -3.70843858e-02  8.91789608e-03  4.78884056e-02 -5.89795075e-02\n",
      "  1.12260051e-01 -6.53123707e-02 -6.84304461e-02 -5.67816980e-02\n",
      "  1.50067871e-02 -2.03076079e-02  9.16799754e-02 -4.07517739e-02\n",
      "  9.18676108e-02  2.31047999e-02  1.31657475e-03  1.83610134e-02\n",
      " -2.33126469e-02  5.43295220e-02 -2.41325628e-02  1.93827990e-02\n",
      " -3.57656479e-02 -3.79402265e-02  2.94638127e-02  4.52185003e-03\n",
      " -5.07431217e-02 -5.43532111e-02 -5.45531698e-02  4.07217890e-02\n",
      "  5.64346323e-03 -7.22044706e-02  3.55672613e-02 -8.98056105e-03\n",
      " -2.72821505e-02 -4.65902546e-03 -4.00151834e-02  9.35490523e-03\n",
      " -2.43916586e-02  5.57625620e-03 -2.48581115e-02 -5.64693566e-03\n",
      " -1.16323624e-02  3.30885611e-02  7.04717170e-03 -6.66778116e-03\n",
      " -3.89769003e-02 -2.68542636e-02  3.29584405e-02  1.02145411e-01\n",
      " -3.34706642e-02 -5.14878053e-03 -6.85933232e-02  4.61427588e-03\n",
      "  2.73172539e-02 -3.39027382e-02 -8.14780220e-02  9.56876203e-04\n",
      "  2.88716494e-03 -6.25763237e-02 -4.49752696e-02 -2.41617579e-03\n",
      "  4.16846238e-02 -6.78773001e-02 -4.87566590e-02 -1.94252580e-02\n",
      "  9.57987085e-02  1.65443402e-02  1.56313963e-02  2.84234919e-02\n",
      "  4.73807976e-02 -4.60582972e-02 -1.54447823e-03  7.37702325e-02\n",
      "  4.65716086e-02  5.03996573e-02  9.30314586e-02  3.74890491e-02\n",
      "  2.87203565e-02 -4.65475023e-02 -2.58788140e-03  9.33464710e-03\n",
      " -6.34837779e-04 -8.29132367e-03  1.76212396e-02 -1.36725018e-02\n",
      " -5.85753657e-02  3.68436575e-02 -4.94690165e-02  1.37639949e-02\n",
      " -4.68045436e-02  2.16336269e-02  1.81567725e-02  4.45702747e-02\n",
      " -6.86840937e-02 -5.90770952e-02  2.09486987e-02  1.40495956e-01\n",
      "  7.61244074e-02 -2.00627502e-02 -1.17114475e-02 -5.25852069e-02\n",
      "  7.51035064e-02  3.36040705e-02 -1.21738836e-02  2.12118798e-03\n",
      "  1.62570775e-02 -3.95544656e-02  7.27340430e-02  6.53061569e-02\n",
      "  1.07511384e-02 -5.06308042e-02  4.03039195e-02 -2.24038698e-02\n",
      " -9.10777301e-02  2.33438872e-02  2.44283453e-02  4.30619911e-33\n",
      " -2.82395445e-02  3.06932582e-03  9.13045034e-02 -5.15592434e-02\n",
      "  1.77311618e-02  5.05933166e-02 -9.58307162e-02  2.63139047e-02\n",
      "  6.72245026e-02 -4.30790484e-02  1.24646835e-02  4.85339575e-02\n",
      " -6.76204264e-02  8.20310339e-02  3.26699764e-02 -5.51248007e-02\n",
      "  1.62476916e-02  3.03732455e-02  6.38661683e-02 -4.93284240e-02\n",
      " -3.38033177e-02 -1.64679307e-02  4.42510992e-02  4.68572080e-02\n",
      "  6.32388575e-04 -1.86084174e-02  3.02629601e-02 -5.92952967e-02\n",
      " -2.31234683e-03  3.04488242e-02  7.30832070e-02 -5.66974888e-03\n",
      " -5.65427132e-02 -1.37735642e-02 -5.36374040e-02  3.17515247e-02\n",
      "  6.84656650e-02 -4.19720113e-02 -1.12932851e-03  1.58316791e-02\n",
      " -7.97236636e-02  6.99080676e-02  1.03657760e-01  3.71871772e-03\n",
      " -8.03296939e-02  8.73310398e-03 -4.72556725e-02  3.54500487e-02\n",
      "  5.39340712e-02  5.08425338e-03 -2.68874299e-02  3.96445133e-02\n",
      " -4.16724011e-02  7.09930509e-02  3.73257250e-02 -7.91060254e-02\n",
      "  3.64444405e-02  3.12701520e-03  7.02943727e-02  7.71185905e-02\n",
      " -8.19326490e-02  2.93834857e-03  1.62032596e-03 -7.05345199e-02\n",
      " -7.27544650e-02  5.79869486e-02 -1.24357259e-02  6.29683435e-02\n",
      " -5.49074672e-02 -3.53063121e-02 -5.63426735e-03 -1.83302164e-02\n",
      "  8.62157568e-02 -9.34635475e-02  3.64200287e-02  7.92979896e-02\n",
      "  6.29263297e-02 -2.62370426e-02 -4.46601845e-02  5.39398231e-02\n",
      "  6.86458778e-03 -5.81653463e-03  4.09722216e-02 -1.14827016e-02\n",
      " -2.91686077e-02 -3.20618935e-02  1.57604199e-02  1.56887770e-02\n",
      "  1.86214633e-02  8.03001300e-02 -1.03444420e-03  2.45437166e-03\n",
      " -6.97502121e-02 -4.69728513e-03 -1.33992909e-02 -2.79354837e-33\n",
      " -6.03049323e-02  5.00690527e-02 -6.17048666e-02  1.04920246e-01\n",
      "  9.93482172e-02 -1.15908943e-01  2.30118036e-02 -2.82698050e-02\n",
      "  1.67954769e-02  1.95211675e-02 -2.91585047e-02 -9.75798164e-03\n",
      " -1.66557971e-02 -6.69068610e-03  9.31934803e-04 -2.47288607e-02\n",
      "  5.79577051e-02 -7.71472603e-02 -2.88796779e-02  4.76449057e-02\n",
      " -5.27006807e-03  3.05719338e-02 -3.86433974e-02 -4.40612435e-02\n",
      "  9.99300741e-03  9.31348801e-02  9.36484896e-03 -7.90352118e-04\n",
      "  4.96952655e-03 -4.79056612e-02 -4.72727679e-02  5.82315447e-03\n",
      " -1.59227233e-02  2.71465722e-02 -7.86102749e-03  7.56470710e-02\n",
      "  1.33744329e-01  4.52296585e-02 -2.38655601e-02  2.07184930e-03\n",
      "  3.93798128e-02 -5.52930795e-02 -4.49035317e-02 -3.66281532e-02\n",
      "  5.60876261e-03  8.65590051e-02 -8.43932331e-02 -3.43624726e-02\n",
      " -1.33920433e-02  2.09494364e-02  7.17280135e-02 -1.24376956e-02\n",
      " -3.32070738e-02 -6.64044172e-02 -3.12024690e-02 -8.68820697e-02\n",
      "  5.46714813e-02  4.35586423e-02 -9.36848968e-02 -5.11399470e-02\n",
      "  8.74101594e-02  4.91915718e-02  5.08603714e-02  8.35484639e-02\n",
      " -5.43855317e-02  2.85048727e-02  1.66489314e-02  2.87035424e-02\n",
      "  5.97144514e-02 -9.44308564e-02 -5.88773116e-02 -7.71072954e-02\n",
      "  4.75300476e-02 -2.60110702e-02 -3.02153751e-02  1.25098586e-01\n",
      "  5.42627275e-02 -6.49609230e-03  2.25316230e-02  6.99130772e-03\n",
      " -5.64274788e-02  2.17078626e-02  6.47219969e-03 -1.04816556e-01\n",
      "  4.65203784e-02 -1.00027055e-01  3.69448997e-02 -4.34254222e-02\n",
      " -2.47894377e-02  1.56856909e-01 -6.63717389e-02 -1.10609154e-03\n",
      " -6.90440610e-02  1.30375139e-02  5.12021817e-02 -3.99453874e-08\n",
      " -7.19047710e-02 -9.99199003e-02  6.05581142e-02  7.96198547e-02\n",
      "  3.21735023e-03 -6.61165714e-02  7.92915300e-02  6.55193031e-02\n",
      "  4.58651669e-02  5.80793321e-02  6.83412477e-02  3.90505046e-03\n",
      "  2.98248976e-02 -2.93370336e-03 -4.23658863e-02  2.98281610e-02\n",
      "  5.62425032e-02 -7.04314634e-02 -8.82187951e-03 -5.40870130e-02\n",
      "  3.08967456e-02  4.60403599e-03  2.00041216e-02 -2.64334306e-02\n",
      " -3.95955481e-02 -1.07212374e-02 -1.36536071e-02  8.08916390e-02\n",
      " -3.96841243e-02 -4.89544086e-02 -2.32719295e-02 -5.98442554e-02\n",
      "  1.47736315e-02  1.04355235e-02 -5.42278914e-03 -6.30957484e-02\n",
      " -1.21761091e-01 -5.61040081e-02 -4.21390496e-02  3.96474861e-02\n",
      " -5.65253422e-02 -8.34429264e-02 -1.00303134e-02 -1.17647396e-02\n",
      "  1.87757122e-03 -1.85979381e-02 -1.33246914e-01 -4.55788895e-03\n",
      "  7.35039171e-03  4.92564179e-02  5.09744063e-02  2.20093057e-02\n",
      "  6.89005479e-02 -3.91468257e-02 -2.19036918e-02  2.06390806e-02\n",
      "  5.14547192e-02 -1.87676456e-02 -2.78329607e-02  1.13911331e-02\n",
      " -2.06624623e-02 -4.30943891e-02 -6.57418147e-02  2.03241520e-02]\n",
      "Vector length: 384\n"
     ]
    }
   ],
   "source": [
    "# Reconnect to ChromaDB collection\n",
    "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "collection = client.get_collection(name=\"posts_collection\")\n",
    "\n",
    "# Fetch a specific post with its embedding\n",
    "result = collection.get(\n",
    "    ids=[\"post_0\"],\n",
    "    include=[\"documents\", \"metadatas\", \"embeddings\"]\n",
    ")\n",
    "\n",
    "# Access the embedding\n",
    "embedding_vector = result[\"embeddings\"][0]\n",
    "\n",
    "# Print part of the embedding for inspection\n",
    "print(\"Embedding vector (first 5 values):\", embedding_vector[:5])\n",
    "print(\"Vector length:\", len(embedding_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d050575",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_name = \"simonsinek\"  # Replace with your desired creator\n",
    "\n",
    "results = collection.get(\n",
    "    where={\"profile_name\": profile_name},\n",
    "    include=[\"documents\", \"metadatas\"]\n",
    ")\n",
    "\n",
    "for i, post in enumerate(results[\"documents\"]):\n",
    "    print(f\"{i+1}. {post}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc5bbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['andrewyng', 'Saurabh Jain', 'simonsinek', 'cassie-kozyrkov-9531919', 'Archit Anand']\n"
     ]
    }
   ],
   "source": [
    "# Fetch all documents in the collection (or in batches if it's large)\n",
    "results = collection.get(include=[\"metadatas\"])\n",
    "\n",
    "# Extract profile names from metadata\n",
    "profile_names = [meta['profile_name'] for meta in results['metadatas']]\n",
    "\n",
    "# Get unique profile names\n",
    "unique_profile_names = list(set(profile_names))\n",
    "\n",
    "print(unique_profile_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d5a9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ profile_name updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Fetch all documents with the old profile_name\n",
    "results = collection.get(\n",
    "    where={\"profile_name\": \"Saurabh Jain\"},\n",
    "    include=[\"documents\", \"metadatas\", \"embeddings\"]\n",
    ")\n",
    "\n",
    "# Step 2: Prepare new metadatas with updated profile_name\n",
    "new_metadatas = []\n",
    "for meta in results['metadatas']:\n",
    "    meta['profile_name'] = \"Saurabh Jain\"\n",
    "    new_metadatas.append(meta)\n",
    "\n",
    "# Step 3: Delete old documents\n",
    "collection.delete(ids=results['ids'])\n",
    "\n",
    "# Step 4: Re-insert with updated profile_name\n",
    "collection.add(\n",
    "    documents=results['documents'],\n",
    "    embeddings=results['embeddings'],\n",
    "    ids=results['ids'],\n",
    "    metadatas=new_metadatas\n",
    ")\n",
    "\n",
    "print(\"✅ profile_name updated successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
